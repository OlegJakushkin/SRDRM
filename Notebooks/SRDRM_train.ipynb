{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SRDRM-train",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c7-Wk-CbhZn",
        "outputId": "8e56b837-a70b-47a1-f253-4ea8994144b0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JE_Khk9SZFRq",
        "outputId": "e8bbf576-e1ee-493c-b268-47611f4b2d61"
      },
      "source": [
        "!git clone --recursive https://github.com/OlegJakushkin/SRDRM\r\n",
        "%cd SRDRM\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SRDRM'...\n",
            "remote: Enumerating objects: 430, done.\u001b[K\n",
            "remote: Counting objects: 100% (430/430), done.\u001b[K\n",
            "remote: Compressing objects: 100% (343/343), done.\u001b[K\n",
            "remote: Total 430 (delta 167), reused 346 (delta 84), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (430/430), 39.35 MiB | 20.42 MiB/s, done.\n",
            "Resolving deltas: 100% (167/167), done.\n",
            "/content/SRDRM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQM7ojqSZeQo"
      },
      "source": [
        "!cp -fr /content/drive/MyDrive/USR-248 ./USR-248\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IJK1EJBhbf1",
        "outputId": "7febdb10-084d-4fbc-a743-aab277b58cb9"
      },
      "source": [
        "!pip uninstall -yq tf-nightly-gpu  tensorflow keras tensorflow-gpu keras-preprocessing keras-applications  scipy\r\n",
        "!pip install tensorflow-gpu==1.13.1\r\n",
        "!pip install numpy keras==2.3 keras-applications==1.0.6 keras-preprocessing==1.0.5 blend_modes youtube_dl pafy natsort  fvcore scipy==1.1.0 deprecated  numpy   blend_modes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping tf-nightly-gpu as it is not installed.\u001b[0m\n",
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n",
            "Collecting tensorflow-gpu==1.13.1\n",
            "  Using cached https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.33.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.35.1)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.18.5)\n",
            "Collecting keras-preprocessing>=1.0.5\n",
            "  Using cached https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.10.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu==1.13.1) (4.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1) (50.3.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.4.0)\n",
            "Installing collected packages: keras-preprocessing, keras-applications, tensorflow-gpu\n",
            "Successfully installed keras-applications-1.0.8 keras-preprocessing-1.1.2 tensorflow-gpu-1.13.1\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Collecting keras==2.3\n",
            "  Using cached https://files.pythonhosted.org/packages/1b/18/2e1ef121e5560ac24c7ac9e363aa5fa7006c40563c989e7211aba95b793a/Keras-2.3.0-py2.py3-none-any.whl\n",
            "Collecting keras-applications==1.0.6\n",
            "  Using cached https://files.pythonhosted.org/packages/3f/c4/2ff40221029f7098d58f8d7fb99b97e8100f3293f9856f0fb5834bef100b/Keras_Applications-1.0.6-py2.py3-none-any.whl\n",
            "Collecting keras-preprocessing==1.0.5\n",
            "  Using cached https://files.pythonhosted.org/packages/fc/94/74e0fa783d3fc07e41715973435dd051ca89c550881b3454233c39c73e69/Keras_Preprocessing-1.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: blend_modes in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Requirement already satisfied: youtube_dl in /usr/local/lib/python3.6/dist-packages (2020.12.9)\n",
            "Requirement already satisfied: pafy in /usr/local/lib/python3.6/dist-packages (0.5.5)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.6/dist-packages (5.5.0)\n",
            "Collecting fvcore\n",
            "  Downloading https://files.pythonhosted.org/packages/a7/a4/031f08369517e71ba056f0692a0262e34e09735bc6ed1392bb09ee85995e/fvcore-0.1.2.post20201209.tar.gz\n",
            "Collecting scipy==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2MB 1.3MB/s \n",
            "\u001b[?25hCollecting deprecated\n",
            "  Downloading https://files.pythonhosted.org/packages/76/a1/05d7f62f956d77b23a640efc650f80ce24483aa2f85a09c03fb64f49e879/Deprecated-1.2.10-py2.py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3) (1.15.0)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/38/4f/fe9a4d472aa867878ce3bb7efb16654c5d63672b86dc0e6e953a67018433/yacs-0.1.8-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fvcore) (4.41.1)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from fvcore) (1.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fvcore) (7.0.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from fvcore) (0.8.7)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated) (1.12.1)\n",
            "Building wheels for collected packages: fvcore\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.2.post20201209-cp36-none-any.whl size=44713 sha256=fe46f574ed761892ee6a50c2cc119243c18f82b48039bd80585de8cc4eccf9c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/91/94/88a21c42c70b94a4f3270a83faca6990a5c14d95a44477af4b\n",
            "Successfully built fvcore\n",
            "\u001b[31mERROR: fancyimpute 0.4.3 requires tensorflow, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fvcore 0.1.2.post20201209 has requirement pyyaml>=5.1, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-preprocessing, keras-applications, scipy, keras, yacs, portalocker, fvcore, deprecated\n",
            "  Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "  Found existing installation: Keras-Applications 1.0.8\n",
            "    Uninstalling Keras-Applications-1.0.8:\n",
            "      Successfully uninstalled Keras-Applications-1.0.8\n",
            "Successfully installed deprecated-1.2.10 fvcore-0.1.2.post20201209 keras-2.3.0 keras-applications-1.0.6 keras-preprocessing-1.0.5 portalocker-2.0.0 scipy-1.1.0 yacs-0.1.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxyHhGXhabsg",
        "outputId": "61f4928f-37dd-4bf5-db11-8839af40d2ee"
      },
      "source": [
        "!python train_GANs_4x.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Loaded 1060 pairs of image-paths for training\n",
            "Using default model: SRDRM-GAN\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "GAN training: srdrm-gan with USR_4x data\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "[Epoch 0: batch 10/265] [d_loss: 0.316038] [g_loss: 0.085570]\n",
            "[Epoch 0: batch 20/265] [d_loss: 0.277816] [g_loss: 0.067798]\n",
            "[Epoch 0: batch 30/265] [d_loss: 0.271565] [g_loss: 0.065706]\n",
            "[Epoch 0: batch 40/265] [d_loss: 0.267226] [g_loss: 0.063765]\n",
            "[Epoch 0: batch 50/265] [d_loss: 0.324789] [g_loss: 0.061870]\n",
            "[Epoch 0: batch 60/265] [d_loss: 0.298097] [g_loss: 0.055905]\n",
            "[Epoch 0: batch 70/265] [d_loss: 0.249665] [g_loss: 0.062521]\n",
            "[Epoch 0: batch 80/265] [d_loss: 0.301764] [g_loss: 0.045708]\n",
            "[Epoch 0: batch 90/265] [d_loss: 0.261175] [g_loss: 0.042301]\n",
            "[Epoch 0: batch 100/265] [d_loss: 0.280492] [g_loss: 0.055156]\n",
            "[Epoch 0: batch 110/265] [d_loss: 0.267748] [g_loss: 0.039916]\n",
            "[Epoch 0: batch 120/265] [d_loss: 0.269971] [g_loss: 0.046158]\n",
            "[Epoch 0: batch 130/265] [d_loss: 0.277242] [g_loss: 0.045431]\n",
            "[Epoch 0: batch 140/265] [d_loss: 0.261976] [g_loss: 0.051216]\n",
            "[Epoch 0: batch 150/265] [d_loss: 0.282424] [g_loss: 0.053519]\n",
            "[Epoch 0: batch 160/265] [d_loss: 0.301987] [g_loss: 0.039394]\n",
            "[Epoch 0: batch 170/265] [d_loss: 0.236111] [g_loss: 0.036926]\n",
            "[Epoch 0: batch 180/265] [d_loss: 0.267437] [g_loss: 0.043604]\n",
            "[Epoch 0: batch 190/265] [d_loss: 0.243805] [g_loss: 0.041344]\n",
            "[Epoch 0: batch 200/265] [d_loss: 0.202641] [g_loss: 0.043171]\n",
            "[Epoch 0: batch 210/265] [d_loss: 0.206350] [g_loss: 0.042026]\n",
            "[Epoch 0: batch 220/265] [d_loss: 0.304339] [g_loss: 0.039395]\n",
            "[Epoch 0: batch 230/265] [d_loss: 0.374904] [g_loss: 0.039477]\n",
            "[Epoch 0: batch 240/265] [d_loss: 0.283791] [g_loss: 0.051511]\n",
            "[Epoch 0: batch 250/265] [d_loss: 0.138092] [g_loss: 0.049606]\n",
            "[Epoch 0: batch 260/265] [d_loss: 0.287501] [g_loss: 0.050616]\n",
            "[Epoch 1: batch 6/265] [d_loss: 0.261040] [g_loss: 0.039648]\n",
            "[Epoch 1: batch 16/265] [d_loss: 0.241227] [g_loss: 0.042530]\n",
            "[Epoch 1: batch 26/265] [d_loss: 0.245596] [g_loss: 0.050085]\n",
            "[Epoch 1: batch 36/265] [d_loss: 0.465611] [g_loss: 0.032205]\n",
            "[Epoch 1: batch 46/265] [d_loss: 0.124508] [g_loss: 0.038370]\n",
            "[Epoch 1: batch 56/265] [d_loss: 0.349951] [g_loss: 0.034430]\n",
            "[Epoch 1: batch 66/265] [d_loss: 0.163616] [g_loss: 0.041181]\n",
            "[Epoch 1: batch 76/265] [d_loss: 0.250554] [g_loss: 0.037684]\n",
            "[Epoch 1: batch 86/265] [d_loss: 0.318830] [g_loss: 0.044107]\n",
            "[Epoch 1: batch 96/265] [d_loss: 0.052662] [g_loss: 0.032228]\n",
            "[Epoch 1: batch 106/265] [d_loss: 0.141118] [g_loss: 0.037884]\n",
            "[Epoch 1: batch 116/265] [d_loss: 0.084291] [g_loss: 0.031328]\n",
            "[Epoch 1: batch 126/265] [d_loss: 0.332801] [g_loss: 0.038051]\n",
            "[Epoch 1: batch 136/265] [d_loss: 0.040438] [g_loss: 0.032243]\n",
            "[Epoch 1: batch 146/265] [d_loss: 0.384225] [g_loss: 0.034918]\n",
            "[Epoch 1: batch 156/265] [d_loss: 0.029842] [g_loss: 0.035832]\n",
            "[Epoch 1: batch 166/265] [d_loss: 0.016674] [g_loss: 0.038980]\n",
            "[Epoch 1: batch 176/265] [d_loss: 0.110754] [g_loss: 0.034152]\n",
            "[Epoch 1: batch 186/265] [d_loss: 0.035519] [g_loss: 0.036520]\n",
            "[Epoch 1: batch 196/265] [d_loss: 0.303057] [g_loss: 0.037057]\n",
            "[Epoch 1: batch 206/265] [d_loss: 0.054591] [g_loss: 0.031356]\n",
            "[Epoch 1: batch 216/265] [d_loss: 0.458287] [g_loss: 0.031236]\n",
            "[Epoch 1: batch 226/265] [d_loss: 0.078714] [g_loss: 0.029960]\n",
            "[Epoch 1: batch 236/265] [d_loss: 0.312295] [g_loss: 0.037480]\n",
            "[Epoch 1: batch 246/265] [d_loss: 0.010286] [g_loss: 0.035735]\n",
            "[Epoch 1: batch 256/265] [d_loss: 0.008949] [g_loss: 0.036659]\n",
            "[Epoch 2: batch 2/265] [d_loss: 0.045231] [g_loss: 0.029484]\n",
            "[Epoch 2: batch 12/265] [d_loss: 0.027101] [g_loss: 0.041557]\n",
            "[Epoch 2: batch 22/265] [d_loss: 0.015068] [g_loss: 0.050298]\n",
            "[Epoch 2: batch 32/265] [d_loss: 0.014413] [g_loss: 0.046190]\n",
            "[Epoch 2: batch 42/265] [d_loss: 0.063374] [g_loss: 0.037152]\n",
            "[Epoch 2: batch 52/265] [d_loss: 0.287511] [g_loss: 0.038991]\n",
            "[Epoch 2: batch 62/265] [d_loss: 0.004964] [g_loss: 0.040642]\n",
            "[Epoch 2: batch 72/265] [d_loss: 0.014734] [g_loss: 0.039148]\n",
            "[Epoch 2: batch 82/265] [d_loss: 0.103777] [g_loss: 0.036654]\n",
            "[Epoch 2: batch 92/265] [d_loss: 0.099309] [g_loss: 0.034826]\n",
            "[Epoch 2: batch 102/265] [d_loss: 0.011940] [g_loss: 0.032400]\n",
            "[Epoch 2: batch 112/265] [d_loss: 0.248987] [g_loss: 0.039018]\n",
            "[Epoch 2: batch 122/265] [d_loss: 0.022943] [g_loss: 0.030496]\n",
            "[Epoch 2: batch 132/265] [d_loss: 0.126779] [g_loss: 0.040841]\n",
            "[Epoch 2: batch 142/265] [d_loss: 0.018081] [g_loss: 0.031463]\n",
            "[Epoch 2: batch 152/265] [d_loss: 0.029428] [g_loss: 0.035220]\n",
            "[Epoch 2: batch 162/265] [d_loss: 0.003246] [g_loss: 0.028001]\n",
            "[Epoch 2: batch 172/265] [d_loss: 0.423872] [g_loss: 0.035033]\n",
            "[Epoch 2: batch 182/265] [d_loss: 0.008678] [g_loss: 0.030993]\n",
            "[Epoch 2: batch 192/265] [d_loss: 0.007711] [g_loss: 0.035361]\n",
            "[Epoch 2: batch 202/265] [d_loss: 0.250965] [g_loss: 0.032519]\n",
            "[Epoch 2: batch 212/265] [d_loss: 0.005008] [g_loss: 0.032218]\n",
            "[Epoch 2: batch 222/265] [d_loss: 0.038246] [g_loss: 0.029062]\n",
            "[Epoch 2: batch 232/265] [d_loss: 0.437296] [g_loss: 0.030445]\n",
            "[Epoch 2: batch 242/265] [d_loss: 0.038253] [g_loss: 0.038137]\n",
            "[Epoch 2: batch 252/265] [d_loss: 0.265271] [g_loss: 0.043533]\n",
            "[Epoch 2: batch 262/265] [d_loss: 0.361690] [g_loss: 0.035452]\n",
            "[Epoch 3: batch 8/265] [d_loss: 0.231202] [g_loss: 0.035979]\n",
            "[Epoch 3: batch 18/265] [d_loss: 0.455233] [g_loss: 0.031756]\n",
            "[Epoch 3: batch 28/265] [d_loss: 0.010894] [g_loss: 0.030865]\n",
            "[Epoch 3: batch 38/265] [d_loss: 0.036156] [g_loss: 0.035703]\n",
            "[Epoch 3: batch 48/265] [d_loss: 0.155458] [g_loss: 0.037036]\n",
            "[Epoch 3: batch 58/265] [d_loss: 0.196381] [g_loss: 0.026403]\n",
            "[Epoch 3: batch 68/265] [d_loss: 0.208351] [g_loss: 0.030639]\n",
            "[Epoch 3: batch 78/265] [d_loss: 0.111117] [g_loss: 0.026940]\n",
            "[Epoch 3: batch 88/265] [d_loss: 0.036064] [g_loss: 0.033862]\n",
            "[Epoch 3: batch 98/265] [d_loss: 0.214144] [g_loss: 0.034085]\n",
            "[Epoch 3: batch 108/265] [d_loss: 0.443461] [g_loss: 0.032402]\n",
            "[Epoch 3: batch 118/265] [d_loss: 0.255305] [g_loss: 0.029811]\n",
            "[Epoch 3: batch 128/265] [d_loss: 0.114308] [g_loss: 0.032059]\n",
            "[Epoch 3: batch 138/265] [d_loss: 0.358448] [g_loss: 0.028694]\n",
            "[Epoch 3: batch 148/265] [d_loss: 0.375439] [g_loss: 0.036026]\n",
            "[Epoch 3: batch 158/265] [d_loss: 0.046087] [g_loss: 0.035486]\n",
            "[Epoch 3: batch 168/265] [d_loss: 0.465067] [g_loss: 0.030049]\n",
            "[Epoch 3: batch 178/265] [d_loss: 0.366360] [g_loss: 0.036228]\n",
            "[Epoch 3: batch 188/265] [d_loss: 0.023785] [g_loss: 0.030819]\n",
            "[Epoch 3: batch 198/265] [d_loss: 0.413615] [g_loss: 0.031425]\n",
            "[Epoch 3: batch 208/265] [d_loss: 0.147966] [g_loss: 0.036252]\n",
            "[Epoch 3: batch 218/265] [d_loss: 0.369228] [g_loss: 0.036610]\n",
            "[Epoch 3: batch 228/265] [d_loss: 0.077336] [g_loss: 0.029919]\n",
            "[Epoch 3: batch 238/265] [d_loss: 0.102296] [g_loss: 0.028808]\n",
            "[Epoch 3: batch 248/265] [d_loss: 0.151969] [g_loss: 0.026821]\n",
            "[Epoch 3: batch 258/265] [d_loss: 0.133802] [g_loss: 0.032044]\n",
            "\n",
            "Saved trained model in /content/drive/MyDrive/SRDRM/checkpoints/USR_4x/srdrm-gan\n",
            "\n",
            "[Epoch 4: batch 4/265] [d_loss: 0.205922] [g_loss: 0.041460]\n",
            "[Epoch 4: batch 14/265] [d_loss: 0.224027] [g_loss: 0.028037]\n",
            "[Epoch 4: batch 24/265] [d_loss: 0.055181] [g_loss: 0.034924]\n",
            "[Epoch 4: batch 34/265] [d_loss: 0.330794] [g_loss: 0.026391]\n",
            "[Epoch 4: batch 44/265] [d_loss: 0.096017] [g_loss: 0.039089]\n",
            "[Epoch 4: batch 54/265] [d_loss: 0.346602] [g_loss: 0.030671]\n",
            "[Epoch 4: batch 64/265] [d_loss: 0.047446] [g_loss: 0.037564]\n",
            "[Epoch 4: batch 74/265] [d_loss: 0.047007] [g_loss: 0.034782]\n",
            "[Epoch 4: batch 84/265] [d_loss: 0.041034] [g_loss: 0.033391]\n",
            "[Epoch 4: batch 94/265] [d_loss: 0.090882] [g_loss: 0.035800]\n",
            "[Epoch 4: batch 104/265] [d_loss: 0.273678] [g_loss: 0.029921]\n",
            "[Epoch 4: batch 114/265] [d_loss: 0.120683] [g_loss: 0.027894]\n",
            "[Epoch 4: batch 124/265] [d_loss: 0.142130] [g_loss: 0.036805]\n",
            "[Epoch 4: batch 134/265] [d_loss: 0.019163] [g_loss: 0.032504]\n",
            "[Epoch 4: batch 144/265] [d_loss: 0.219375] [g_loss: 0.025581]\n",
            "[Epoch 4: batch 154/265] [d_loss: 0.192471] [g_loss: 0.034318]\n",
            "[Epoch 4: batch 164/265] [d_loss: 0.197533] [g_loss: 0.032283]\n",
            "[Epoch 4: batch 174/265] [d_loss: 0.122019] [g_loss: 0.036137]\n",
            "[Epoch 4: batch 184/265] [d_loss: 0.095780] [g_loss: 0.040983]\n",
            "[Epoch 4: batch 194/265] [d_loss: 0.162698] [g_loss: 0.036090]\n",
            "[Epoch 4: batch 204/265] [d_loss: 0.306400] [g_loss: 0.044181]\n",
            "[Epoch 4: batch 214/265] [d_loss: 0.009412] [g_loss: 0.029025]\n",
            "[Epoch 4: batch 224/265] [d_loss: 0.003765] [g_loss: 0.032552]\n",
            "[Epoch 4: batch 234/265] [d_loss: 0.358905] [g_loss: 0.030935]\n",
            "[Epoch 4: batch 244/265] [d_loss: 0.248193] [g_loss: 0.033407]\n",
            "[Epoch 4: batch 254/265] [d_loss: 0.101262] [g_loss: 0.031730]\n",
            "[Epoch 4: batch 264/265] [d_loss: 0.298189] [g_loss: 0.043242]\n",
            "[Epoch 5: batch 10/265] [d_loss: 0.297636] [g_loss: 0.040217]\n",
            "[Epoch 5: batch 20/265] [d_loss: 0.281576] [g_loss: 0.032788]\n",
            "[Epoch 5: batch 30/265] [d_loss: 0.062565] [g_loss: 0.033342]\n",
            "[Epoch 5: batch 40/265] [d_loss: 0.283328] [g_loss: 0.030149]\n",
            "[Epoch 5: batch 50/265] [d_loss: 0.087171] [g_loss: 0.036672]\n",
            "[Epoch 5: batch 60/265] [d_loss: 0.212888] [g_loss: 0.032989]\n",
            "[Epoch 5: batch 70/265] [d_loss: 0.153680] [g_loss: 0.037898]\n",
            "[Epoch 5: batch 80/265] [d_loss: 0.235197] [g_loss: 0.029118]\n",
            "[Epoch 5: batch 90/265] [d_loss: 0.321338] [g_loss: 0.028743]\n",
            "[Epoch 5: batch 100/265] [d_loss: 0.329297] [g_loss: 0.034983]\n",
            "[Epoch 5: batch 110/265] [d_loss: 0.065565] [g_loss: 0.030218]\n",
            "[Epoch 5: batch 120/265] [d_loss: 0.106572] [g_loss: 0.033759]\n",
            "[Epoch 5: batch 130/265] [d_loss: 0.166428] [g_loss: 0.034645]\n",
            "[Epoch 5: batch 140/265] [d_loss: 0.393217] [g_loss: 0.036410]\n",
            "[Epoch 5: batch 150/265] [d_loss: 0.217536] [g_loss: 0.039454]\n",
            "[Epoch 5: batch 160/265] [d_loss: 0.438050] [g_loss: 0.029709]\n",
            "[Epoch 5: batch 170/265] [d_loss: 0.359706] [g_loss: 0.028446]\n",
            "[Epoch 5: batch 180/265] [d_loss: 0.098208] [g_loss: 0.034432]\n",
            "[Epoch 5: batch 190/265] [d_loss: 0.208949] [g_loss: 0.032306]\n",
            "[Epoch 5: batch 200/265] [d_loss: 0.294353] [g_loss: 0.034595]\n",
            "[Epoch 5: batch 210/265] [d_loss: 0.077109] [g_loss: 0.034796]\n",
            "[Epoch 5: batch 220/265] [d_loss: 0.394953] [g_loss: 0.029646]\n",
            "[Epoch 5: batch 230/265] [d_loss: 0.415621] [g_loss: 0.031835]\n",
            "[Epoch 5: batch 240/265] [d_loss: 0.167990] [g_loss: 0.032547]\n",
            "[Epoch 5: batch 250/265] [d_loss: 0.073831] [g_loss: 0.037922]\n",
            "[Epoch 5: batch 260/265] [d_loss: 0.332607] [g_loss: 0.038598]\n",
            "[Epoch 6: batch 6/265] [d_loss: 0.031474] [g_loss: 0.031973]\n",
            "[Epoch 6: batch 16/265] [d_loss: 0.354825] [g_loss: 0.034938]\n",
            "[Epoch 6: batch 26/265] [d_loss: 0.132282] [g_loss: 0.044202]\n",
            "[Epoch 6: batch 36/265] [d_loss: 0.292784] [g_loss: 0.027850]\n",
            "[Epoch 6: batch 46/265] [d_loss: 0.147411] [g_loss: 0.036000]\n",
            "[Epoch 6: batch 56/265] [d_loss: 0.072477] [g_loss: 0.029924]\n",
            "[Epoch 6: batch 66/265] [d_loss: 0.317680] [g_loss: 0.031495]\n",
            "[Epoch 6: batch 76/265] [d_loss: 0.061414] [g_loss: 0.032734]\n",
            "[Epoch 6: batch 86/265] [d_loss: 0.022689] [g_loss: 0.037983]\n",
            "[Epoch 6: batch 96/265] [d_loss: 0.023661] [g_loss: 0.024295]\n",
            "[Epoch 6: batch 106/265] [d_loss: 0.224321] [g_loss: 0.030343]\n",
            "[Epoch 6: batch 116/265] [d_loss: 0.297908] [g_loss: 0.024788]\n",
            "[Epoch 6: batch 126/265] [d_loss: 0.423181] [g_loss: 0.034490]\n",
            "[Epoch 6: batch 136/265] [d_loss: 0.267552] [g_loss: 0.029285]\n",
            "[Epoch 6: batch 146/265] [d_loss: 0.180201] [g_loss: 0.032714]\n",
            "[Epoch 6: batch 156/265] [d_loss: 0.165452] [g_loss: 0.030119]\n",
            "[Epoch 6: batch 166/265] [d_loss: 0.067256] [g_loss: 0.034596]\n",
            "[Epoch 6: batch 176/265] [d_loss: 0.023048] [g_loss: 0.029724]\n",
            "[Epoch 6: batch 186/265] [d_loss: 0.040344] [g_loss: 0.030919]\n",
            "[Epoch 6: batch 196/265] [d_loss: 0.033672] [g_loss: 0.031876]\n",
            "[Epoch 6: batch 206/265] [d_loss: 0.427091] [g_loss: 0.031204]\n",
            "[Epoch 6: batch 216/265] [d_loss: 0.044365] [g_loss: 0.026158]\n",
            "[Epoch 6: batch 226/265] [d_loss: 0.535798] [g_loss: 0.025134]\n",
            "[Epoch 6: batch 236/265] [d_loss: 0.197008] [g_loss: 0.033359]\n",
            "[Epoch 6: batch 246/265] [d_loss: 0.092817] [g_loss: 0.029390]\n",
            "[Epoch 6: batch 256/265] [d_loss: 0.183608] [g_loss: 0.028565]\n",
            "[Epoch 7: batch 2/265] [d_loss: 0.110549] [g_loss: 0.026097]\n",
            "[Epoch 7: batch 12/265] [d_loss: 0.148842] [g_loss: 0.028274]\n",
            "[Epoch 7: batch 22/265] [d_loss: 0.249656] [g_loss: 0.041027]\n",
            "[Epoch 7: batch 32/265] [d_loss: 0.217704] [g_loss: 0.035931]\n",
            "[Epoch 7: batch 42/265] [d_loss: 0.295887] [g_loss: 0.027742]\n",
            "[Epoch 7: batch 52/265] [d_loss: 0.276676] [g_loss: 0.031413]\n",
            "[Epoch 7: batch 62/265] [d_loss: 0.154946] [g_loss: 0.033606]\n",
            "[Epoch 7: batch 72/265] [d_loss: 0.063309] [g_loss: 0.028830]\n",
            "[Epoch 7: batch 82/265] [d_loss: 0.213703] [g_loss: 0.030761]\n",
            "[Epoch 7: batch 92/265] [d_loss: 0.159208] [g_loss: 0.028980]\n",
            "[Epoch 7: batch 102/265] [d_loss: 0.212149] [g_loss: 0.027905]\n",
            "[Epoch 7: batch 112/265] [d_loss: 0.072390] [g_loss: 0.034672]\n",
            "[Epoch 7: batch 122/265] [d_loss: 0.030561] [g_loss: 0.025802]\n",
            "[Epoch 7: batch 132/265] [d_loss: 0.136157] [g_loss: 0.035043]\n",
            "[Epoch 7: batch 142/265] [d_loss: 0.189717] [g_loss: 0.026450]\n",
            "[Epoch 7: batch 152/265] [d_loss: 0.356899] [g_loss: 0.032240]\n",
            "[Epoch 7: batch 162/265] [d_loss: 0.145373] [g_loss: 0.023700]\n",
            "[Epoch 7: batch 172/265] [d_loss: 0.036758] [g_loss: 0.031265]\n",
            "[Epoch 7: batch 182/265] [d_loss: 0.053190] [g_loss: 0.027214]\n",
            "[Epoch 7: batch 192/265] [d_loss: 0.099128] [g_loss: 0.031641]\n",
            "[Epoch 7: batch 202/265] [d_loss: 0.383967] [g_loss: 0.028208]\n",
            "[Epoch 7: batch 212/265] [d_loss: 0.160390] [g_loss: 0.027099]\n",
            "[Epoch 7: batch 222/265] [d_loss: 0.029151] [g_loss: 0.026361]\n",
            "[Epoch 7: batch 232/265] [d_loss: 0.321929] [g_loss: 0.030119]\n",
            "[Epoch 7: batch 242/265] [d_loss: 0.085133] [g_loss: 0.035231]\n",
            "[Epoch 7: batch 252/265] [d_loss: 0.073200] [g_loss: 0.040264]\n",
            "[Epoch 7: batch 262/265] [d_loss: 0.324337] [g_loss: 0.030372]\n",
            "\n",
            "Saved trained model in /content/drive/MyDrive/SRDRM/checkpoints/USR_4x/srdrm-gan\n",
            "\n",
            "[Epoch 8: batch 8/265] [d_loss: 0.301660] [g_loss: 0.034720]\n",
            "[Epoch 8: batch 18/265] [d_loss: 0.571247] [g_loss: 0.025457]\n",
            "[Epoch 8: batch 28/265] [d_loss: 0.291880] [g_loss: 0.025829]\n",
            "[Epoch 8: batch 38/265] [d_loss: 0.235388] [g_loss: 0.030799]\n",
            "[Epoch 8: batch 48/265] [d_loss: 0.217247] [g_loss: 0.034239]\n",
            "[Epoch 8: batch 58/265] [d_loss: 0.354924] [g_loss: 0.024343]\n",
            "[Epoch 8: batch 68/265] [d_loss: 0.129391] [g_loss: 0.024924]\n",
            "[Epoch 8: batch 78/265] [d_loss: 0.486464] [g_loss: 0.023773]\n",
            "[Epoch 8: batch 88/265] [d_loss: 0.235376] [g_loss: 0.029709]\n",
            "[Epoch 8: batch 98/265] [d_loss: 0.241783] [g_loss: 0.029898]\n",
            "[Epoch 8: batch 108/265] [d_loss: 0.269177] [g_loss: 0.027044]\n",
            "[Epoch 8: batch 118/265] [d_loss: 0.177613] [g_loss: 0.025923]\n",
            "[Epoch 8: batch 128/265] [d_loss: 0.038671] [g_loss: 0.027764]\n",
            "[Epoch 8: batch 138/265] [d_loss: 0.300888] [g_loss: 0.026235]\n",
            "[Epoch 8: batch 148/265] [d_loss: 0.188351] [g_loss: 0.034538]\n",
            "[Epoch 8: batch 158/265] [d_loss: 0.236893] [g_loss: 0.033215]\n",
            "[Epoch 8: batch 168/265] [d_loss: 0.353660] [g_loss: 0.025709]\n",
            "[Epoch 8: batch 178/265] [d_loss: 0.326288] [g_loss: 0.034241]\n",
            "[Epoch 8: batch 188/265] [d_loss: 0.018996] [g_loss: 0.027762]\n",
            "[Epoch 8: batch 198/265] [d_loss: 0.452747] [g_loss: 0.030677]\n",
            "[Epoch 8: batch 208/265] [d_loss: 0.354365] [g_loss: 0.034567]\n",
            "[Epoch 8: batch 218/265] [d_loss: 0.625602] [g_loss: 0.032644]\n",
            "[Epoch 8: batch 228/265] [d_loss: 0.097819] [g_loss: 0.027718]\n",
            "[Epoch 8: batch 238/265] [d_loss: 0.270770] [g_loss: 0.026255]\n",
            "[Epoch 8: batch 248/265] [d_loss: 0.084018] [g_loss: 0.023748]\n",
            "[Epoch 8: batch 258/265] [d_loss: 0.274461] [g_loss: 0.030454]\n",
            "[Epoch 9: batch 4/265] [d_loss: 0.092705] [g_loss: 0.036263]\n",
            "[Epoch 9: batch 14/265] [d_loss: 0.120295] [g_loss: 0.025409]\n",
            "[Epoch 9: batch 24/265] [d_loss: 0.202860] [g_loss: 0.032319]\n",
            "[Epoch 9: batch 34/265] [d_loss: 0.375939] [g_loss: 0.025549]\n",
            "[Epoch 9: batch 44/265] [d_loss: 0.081023] [g_loss: 0.035402]\n",
            "[Epoch 9: batch 54/265] [d_loss: 0.327764] [g_loss: 0.027607]\n",
            "[Epoch 9: batch 64/265] [d_loss: 0.115394] [g_loss: 0.033546]\n",
            "[Epoch 9: batch 74/265] [d_loss: 0.296597] [g_loss: 0.032851]\n",
            "[Epoch 9: batch 84/265] [d_loss: 0.163979] [g_loss: 0.030317]\n",
            "[Epoch 9: batch 94/265] [d_loss: 0.029001] [g_loss: 0.031957]\n",
            "[Epoch 9: batch 104/265] [d_loss: 0.074815] [g_loss: 0.031941]\n",
            "[Epoch 9: batch 114/265] [d_loss: 0.004703] [g_loss: 0.025495]\n",
            "[Epoch 9: batch 124/265] [d_loss: 0.144334] [g_loss: 0.035572]\n",
            "[Epoch 9: batch 134/265] [d_loss: 0.046475] [g_loss: 0.031704]\n",
            "[Epoch 9: batch 144/265] [d_loss: 0.582422] [g_loss: 0.025949]\n",
            "[Epoch 9: batch 154/265] [d_loss: 0.012834] [g_loss: 0.032003]\n",
            "[Epoch 9: batch 164/265] [d_loss: 0.027320] [g_loss: 0.029389]\n",
            "[Epoch 9: batch 174/265] [d_loss: 0.303086] [g_loss: 0.033713]\n",
            "[Epoch 9: batch 184/265] [d_loss: 0.015741] [g_loss: 0.037212]\n",
            "[Epoch 9: batch 194/265] [d_loss: 0.317611] [g_loss: 0.033341]\n",
            "[Epoch 9: batch 204/265] [d_loss: 0.055676] [g_loss: 0.039323]\n",
            "[Epoch 9: batch 214/265] [d_loss: 0.193367] [g_loss: 0.029061]\n",
            "[Epoch 9: batch 224/265] [d_loss: 0.025022] [g_loss: 0.029493]\n",
            "[Epoch 9: batch 234/265] [d_loss: 0.410579] [g_loss: 0.030362]\n",
            "[Epoch 9: batch 244/265] [d_loss: 0.184018] [g_loss: 0.029723]\n",
            "[Epoch 9: batch 254/265] [d_loss: 0.064381] [g_loss: 0.030661]\n",
            "[Epoch 9: batch 264/265] [d_loss: 0.152688] [g_loss: 0.040643]\n",
            "[Epoch 10: batch 10/265] [d_loss: 0.314703] [g_loss: 0.037610]\n",
            "[Epoch 10: batch 20/265] [d_loss: 0.032765] [g_loss: 0.029290]\n",
            "[Epoch 10: batch 30/265] [d_loss: 0.262676] [g_loss: 0.030689]\n",
            "[Epoch 10: batch 40/265] [d_loss: 0.301708] [g_loss: 0.028378]\n",
            "[Epoch 10: batch 50/265] [d_loss: 0.047067] [g_loss: 0.035910]\n",
            "[Epoch 10: batch 60/265] [d_loss: 0.074732] [g_loss: 0.030675]\n",
            "[Epoch 10: batch 70/265] [d_loss: 0.047412] [g_loss: 0.037265]\n",
            "[Epoch 10: batch 80/265] [d_loss: 0.324634] [g_loss: 0.029438]\n",
            "[Epoch 10: batch 90/265] [d_loss: 0.331029] [g_loss: 0.026454]\n",
            "[Epoch 10: batch 100/265] [d_loss: 0.151084] [g_loss: 0.032550]\n",
            "[Epoch 10: batch 110/265] [d_loss: 0.044658] [g_loss: 0.027802]\n",
            "[Epoch 10: batch 120/265] [d_loss: 0.230126] [g_loss: 0.032394]\n",
            "[Epoch 10: batch 130/265] [d_loss: 0.217387] [g_loss: 0.032421]\n",
            "[Epoch 10: batch 140/265] [d_loss: 0.381497] [g_loss: 0.035325]\n",
            "[Epoch 10: batch 150/265] [d_loss: 0.073204] [g_loss: 0.037648]\n",
            "[Epoch 10: batch 160/265] [d_loss: 0.364566] [g_loss: 0.028361]\n",
            "[Epoch 10: batch 170/265] [d_loss: 0.091288] [g_loss: 0.027681]\n",
            "[Epoch 10: batch 180/265] [d_loss: 0.007952] [g_loss: 0.031848]\n",
            "[Epoch 10: batch 190/265] [d_loss: 0.126530] [g_loss: 0.030175]\n",
            "[Epoch 10: batch 200/265] [d_loss: 0.047424] [g_loss: 0.032295]\n",
            "[Epoch 10: batch 210/265] [d_loss: 0.124061] [g_loss: 0.033567]\n",
            "[Epoch 10: batch 220/265] [d_loss: 0.262654] [g_loss: 0.029274]\n",
            "[Epoch 10: batch 230/265] [d_loss: 0.393016] [g_loss: 0.028888]\n",
            "[Epoch 10: batch 240/265] [d_loss: 0.140392] [g_loss: 0.032046]\n",
            "[Epoch 10: batch 250/265] [d_loss: 0.308558] [g_loss: 0.037739]\n",
            "[Epoch 10: batch 260/265] [d_loss: 0.290151] [g_loss: 0.037146]\n",
            "[Epoch 11: batch 6/265] [d_loss: 0.079661] [g_loss: 0.029993]\n",
            "[Epoch 11: batch 16/265] [d_loss: 0.333753] [g_loss: 0.032568]\n",
            "[Epoch 11: batch 26/265] [d_loss: 0.145543] [g_loss: 0.042144]\n",
            "[Epoch 11: batch 36/265] [d_loss: 0.127295] [g_loss: 0.025674]\n",
            "[Epoch 11: batch 46/265] [d_loss: 0.059551] [g_loss: 0.035245]\n",
            "[Epoch 11: batch 56/265] [d_loss: 0.064037] [g_loss: 0.027671]\n",
            "[Epoch 11: batch 66/265] [d_loss: 0.128903] [g_loss: 0.030115]\n",
            "[Epoch 11: batch 76/265] [d_loss: 0.022587] [g_loss: 0.031565]\n",
            "[Epoch 11: batch 86/265] [d_loss: 0.278042] [g_loss: 0.039585]\n",
            "[Epoch 11: batch 96/265] [d_loss: 0.112946] [g_loss: 0.024846]\n",
            "[Epoch 11: batch 106/265] [d_loss: 0.237466] [g_loss: 0.030484]\n",
            "[Epoch 11: batch 116/265] [d_loss: 0.149328] [g_loss: 0.022952]\n",
            "[Epoch 11: batch 126/265] [d_loss: 0.418641] [g_loss: 0.033278]\n",
            "[Epoch 11: batch 136/265] [d_loss: 0.285723] [g_loss: 0.028411]\n",
            "[Epoch 11: batch 146/265] [d_loss: 0.280127] [g_loss: 0.030596]\n",
            "[Epoch 11: batch 156/265] [d_loss: 0.034072] [g_loss: 0.028252]\n",
            "[Epoch 11: batch 166/265] [d_loss: 0.007279] [g_loss: 0.033875]\n",
            "[Epoch 11: batch 176/265] [d_loss: 0.096211] [g_loss: 0.028703]\n",
            "[Epoch 11: batch 186/265] [d_loss: 0.082247] [g_loss: 0.029109]\n",
            "[Epoch 11: batch 196/265] [d_loss: 0.184285] [g_loss: 0.029394]\n",
            "[Epoch 11: batch 206/265] [d_loss: 0.451101] [g_loss: 0.030520]\n",
            "[Epoch 11: batch 216/265] [d_loss: 0.161965] [g_loss: 0.024799]\n",
            "[Epoch 11: batch 226/265] [d_loss: 0.047588] [g_loss: 0.024879]\n",
            "[Epoch 11: batch 236/265] [d_loss: 0.071401] [g_loss: 0.032015]\n",
            "[Epoch 11: batch 246/265] [d_loss: 0.007682] [g_loss: 0.030015]\n",
            "[Epoch 11: batch 256/265] [d_loss: 0.354186] [g_loss: 0.028092]\n",
            "\n",
            "Saved trained model in /content/drive/MyDrive/SRDRM/checkpoints/USR_4x/srdrm-gan\n",
            "\n",
            "[Epoch 12: batch 2/265] [d_loss: 0.339359] [g_loss: 0.026696]\n",
            "[Epoch 12: batch 12/265] [d_loss: 0.183367] [g_loss: 0.028586]\n",
            "[Epoch 12: batch 22/265] [d_loss: 0.175923] [g_loss: 0.036866]\n",
            "[Epoch 12: batch 32/265] [d_loss: 0.056803] [g_loss: 0.032690]\n",
            "[Epoch 12: batch 42/265] [d_loss: 0.168278] [g_loss: 0.026819]\n",
            "[Epoch 12: batch 52/265] [d_loss: 0.287583] [g_loss: 0.030425]\n",
            "[Epoch 12: batch 62/265] [d_loss: 0.136070] [g_loss: 0.033183]\n",
            "[Epoch 12: batch 72/265] [d_loss: 0.102566] [g_loss: 0.027682]\n",
            "[Epoch 12: batch 82/265] [d_loss: 0.308137] [g_loss: 0.029356]\n",
            "[Epoch 12: batch 92/265] [d_loss: 0.148876] [g_loss: 0.026774]\n",
            "[Epoch 12: batch 102/265] [d_loss: 0.228830] [g_loss: 0.027491]\n",
            "[Epoch 12: batch 112/265] [d_loss: 0.295537] [g_loss: 0.033575]\n",
            "[Epoch 12: batch 122/265] [d_loss: 0.076256] [g_loss: 0.024966]\n",
            "[Epoch 12: batch 132/265] [d_loss: 0.186582] [g_loss: 0.033022]\n",
            "[Epoch 12: batch 142/265] [d_loss: 0.350184] [g_loss: 0.026636]\n",
            "[Epoch 12: batch 152/265] [d_loss: 0.033678] [g_loss: 0.030068]\n",
            "[Epoch 12: batch 162/265] [d_loss: 0.062507] [g_loss: 0.023522]\n",
            "[Epoch 12: batch 172/265] [d_loss: 0.047747] [g_loss: 0.031792]\n",
            "[Epoch 12: batch 182/265] [d_loss: 0.014322] [g_loss: 0.026015]\n",
            "[Epoch 12: batch 192/265] [d_loss: 0.388698] [g_loss: 0.030857]\n",
            "[Epoch 12: batch 202/265] [d_loss: 0.373278] [g_loss: 0.028369]\n",
            "[Epoch 12: batch 212/265] [d_loss: 0.286157] [g_loss: 0.024234]\n",
            "[Epoch 12: batch 222/265] [d_loss: 0.260342] [g_loss: 0.024521]\n",
            "[Epoch 12: batch 232/265] [d_loss: 0.125324] [g_loss: 0.029709]\n",
            "[Epoch 12: batch 242/265] [d_loss: 0.057737] [g_loss: 0.033041]\n",
            "[Epoch 12: batch 252/265] [d_loss: 0.067268] [g_loss: 0.038068]\n",
            "[Epoch 12: batch 262/265] [d_loss: 0.177377] [g_loss: 0.030631]\n",
            "[Epoch 13: batch 8/265] [d_loss: 0.339220] [g_loss: 0.032854]\n",
            "[Epoch 13: batch 18/265] [d_loss: 0.136994] [g_loss: 0.025539]\n",
            "[Epoch 13: batch 28/265] [d_loss: 0.365788] [g_loss: 0.026091]\n",
            "[Epoch 13: batch 38/265] [d_loss: 0.144937] [g_loss: 0.031099]\n",
            "[Epoch 13: batch 48/265] [d_loss: 0.137397] [g_loss: 0.032131]\n",
            "[Epoch 13: batch 58/265] [d_loss: 0.401132] [g_loss: 0.023880]\n",
            "[Epoch 13: batch 68/265] [d_loss: 0.035007] [g_loss: 0.022772]\n",
            "[Epoch 13: batch 78/265] [d_loss: 0.356266] [g_loss: 0.023471]\n",
            "[Epoch 13: batch 88/265] [d_loss: 0.087105] [g_loss: 0.029987]\n",
            "[Epoch 13: batch 98/265] [d_loss: 0.223110] [g_loss: 0.030698]\n",
            "[Epoch 13: batch 108/265] [d_loss: 0.390436] [g_loss: 0.028186]\n",
            "[Epoch 13: batch 118/265] [d_loss: 0.043732] [g_loss: 0.025898]\n",
            "[Epoch 13: batch 128/265] [d_loss: 0.085462] [g_loss: 0.028255]\n",
            "[Epoch 13: batch 138/265] [d_loss: 0.253146] [g_loss: 0.025157]\n",
            "[Epoch 13: batch 148/265] [d_loss: 0.354172] [g_loss: 0.032325]\n",
            "[Epoch 13: batch 158/265] [d_loss: 0.139864] [g_loss: 0.031866]\n",
            "[Epoch 13: batch 168/265] [d_loss: 0.438187] [g_loss: 0.028340]\n",
            "[Epoch 13: batch 178/265] [d_loss: 0.261217] [g_loss: 0.033860]\n",
            "[Epoch 13: batch 188/265] [d_loss: 0.027831] [g_loss: 0.028883]\n",
            "[Epoch 13: batch 198/265] [d_loss: 0.171232] [g_loss: 0.031322]\n",
            "[Epoch 13: batch 208/265] [d_loss: 0.264032] [g_loss: 0.036008]\n",
            "[Epoch 13: batch 218/265] [d_loss: 0.413962] [g_loss: 0.032994]\n",
            "[Epoch 13: batch 228/265] [d_loss: 0.048717] [g_loss: 0.029321]\n",
            "[Epoch 13: batch 238/265] [d_loss: 0.034617] [g_loss: 0.030991]\n",
            "[Epoch 13: batch 248/265] [d_loss: 0.022750] [g_loss: 0.025550]\n",
            "[Epoch 13: batch 258/265] [d_loss: 0.415529] [g_loss: 0.028668]\n",
            "[Epoch 14: batch 4/265] [d_loss: 0.117438] [g_loss: 0.037037]\n",
            "[Epoch 14: batch 14/265] [d_loss: 0.259997] [g_loss: 0.025072]\n",
            "[Epoch 14: batch 24/265] [d_loss: 0.052032] [g_loss: 0.030692]\n",
            "[Epoch 14: batch 34/265] [d_loss: 0.379680] [g_loss: 0.025602]\n",
            "[Epoch 14: batch 44/265] [d_loss: 0.225821] [g_loss: 0.034450]\n",
            "[Epoch 14: batch 54/265] [d_loss: 0.382823] [g_loss: 0.026835]\n",
            "[Epoch 14: batch 64/265] [d_loss: 0.054251] [g_loss: 0.036210]\n",
            "[Epoch 14: batch 74/265] [d_loss: 0.042947] [g_loss: 0.034427]\n",
            "[Epoch 14: batch 84/265] [d_loss: 0.325751] [g_loss: 0.029270]\n",
            "[Epoch 14: batch 94/265] [d_loss: 0.089089] [g_loss: 0.030523]\n",
            "[Epoch 14: batch 104/265] [d_loss: 0.306608] [g_loss: 0.027385]\n",
            "[Epoch 14: batch 114/265] [d_loss: 0.017286] [g_loss: 0.025701]\n",
            "[Epoch 14: batch 124/265] [d_loss: 0.128243] [g_loss: 0.032869]\n",
            "[Epoch 14: batch 134/265] [d_loss: 0.052607] [g_loss: 0.030255]\n",
            "[Epoch 14: batch 144/265] [d_loss: 0.189869] [g_loss: 0.024511]\n",
            "[Epoch 14: batch 154/265] [d_loss: 0.023090] [g_loss: 0.031092]\n",
            "[Epoch 14: batch 164/265] [d_loss: 0.045623] [g_loss: 0.027793]\n",
            "[Epoch 14: batch 174/265] [d_loss: 0.275639] [g_loss: 0.033347]\n",
            "[Epoch 14: batch 184/265] [d_loss: 0.033012] [g_loss: 0.038433]\n",
            "[Epoch 14: batch 194/265] [d_loss: 0.098812] [g_loss: 0.034215]\n",
            "[Epoch 14: batch 204/265] [d_loss: 0.015446] [g_loss: 0.040351]\n",
            "[Epoch 14: batch 214/265] [d_loss: 0.022139] [g_loss: 0.027227]\n",
            "[Epoch 14: batch 224/265] [d_loss: 0.103244] [g_loss: 0.029903]\n",
            "[Epoch 14: batch 234/265] [d_loss: 0.152785] [g_loss: 0.029220]\n",
            "[Epoch 14: batch 244/265] [d_loss: 0.033730] [g_loss: 0.031027]\n",
            "[Epoch 14: batch 254/265] [d_loss: 0.019863] [g_loss: 0.030892]\n",
            "[Epoch 14: batch 264/265] [d_loss: 0.070532] [g_loss: 0.041053]\n",
            "[Epoch 15: batch 10/265] [d_loss: 0.007587] [g_loss: 0.037942]\n",
            "[Epoch 15: batch 20/265] [d_loss: 0.005307] [g_loss: 0.030736]\n",
            "[Epoch 15: batch 30/265] [d_loss: 0.005803] [g_loss: 0.032165]\n",
            "[Epoch 15: batch 40/265] [d_loss: 0.035877] [g_loss: 0.029483]\n",
            "[Epoch 15: batch 50/265] [d_loss: 0.051932] [g_loss: 0.036186]\n",
            "[Epoch 15: batch 60/265] [d_loss: 0.043208] [g_loss: 0.032718]\n",
            "[Epoch 15: batch 70/265] [d_loss: 0.001801] [g_loss: 0.039718]\n",
            "[Epoch 15: batch 80/265] [d_loss: 0.009687] [g_loss: 0.032454]\n",
            "[Epoch 15: batch 90/265] [d_loss: 0.002180] [g_loss: 0.028847]\n",
            "[Epoch 15: batch 100/265] [d_loss: 0.154096] [g_loss: 0.031693]\n",
            "[Epoch 15: batch 110/265] [d_loss: 0.120828] [g_loss: 0.027973]\n",
            "[Epoch 15: batch 120/265] [d_loss: 0.366279] [g_loss: 0.033434]\n",
            "[Epoch 15: batch 130/265] [d_loss: 0.018022] [g_loss: 0.034870]\n",
            "[Epoch 15: batch 140/265] [d_loss: 0.482524] [g_loss: 0.034943]\n",
            "[Epoch 15: batch 150/265] [d_loss: 0.330358] [g_loss: 0.036527]\n",
            "[Epoch 15: batch 160/265] [d_loss: 0.308563] [g_loss: 0.029336]\n",
            "[Epoch 15: batch 170/265] [d_loss: 0.071178] [g_loss: 0.028398]\n",
            "[Epoch 15: batch 180/265] [d_loss: 0.041900] [g_loss: 0.031855]\n",
            "[Epoch 15: batch 190/265] [d_loss: 0.437150] [g_loss: 0.028167]\n",
            "[Epoch 15: batch 200/265] [d_loss: 0.195187] [g_loss: 0.032458]\n",
            "[Epoch 15: batch 210/265] [d_loss: 0.050300] [g_loss: 0.034295]\n",
            "[Epoch 15: batch 220/265] [d_loss: 0.476992] [g_loss: 0.032915]\n",
            "[Epoch 15: batch 230/265] [d_loss: 0.450595] [g_loss: 0.027834]\n",
            "[Epoch 15: batch 240/265] [d_loss: 0.290887] [g_loss: 0.031675]\n",
            "[Epoch 15: batch 250/265] [d_loss: 0.347801] [g_loss: 0.036224]\n",
            "[Epoch 15: batch 260/265] [d_loss: 0.375760] [g_loss: 0.039443]\n",
            "\n",
            "Saved trained model in /content/drive/MyDrive/SRDRM/checkpoints/USR_4x/srdrm-gan\n",
            "\n",
            "[Epoch 16: batch 6/265] [d_loss: 0.012956] [g_loss: 0.030377]\n",
            "[Epoch 16: batch 16/265] [d_loss: 0.072093] [g_loss: 0.033222]\n",
            "[Epoch 16: batch 26/265] [d_loss: 0.157258] [g_loss: 0.044708]\n",
            "[Epoch 16: batch 36/265] [d_loss: 0.284968] [g_loss: 0.025810]\n",
            "[Epoch 16: batch 46/265] [d_loss: 0.107394] [g_loss: 0.034546]\n",
            "[Epoch 16: batch 56/265] [d_loss: 0.296935] [g_loss: 0.026601]\n",
            "[Epoch 16: batch 66/265] [d_loss: 0.385488] [g_loss: 0.030780]\n",
            "[Epoch 16: batch 76/265] [d_loss: 0.076203] [g_loss: 0.033150]\n",
            "[Epoch 16: batch 86/265] [d_loss: 0.244199] [g_loss: 0.037694]\n",
            "[Epoch 16: batch 96/265] [d_loss: 0.033798] [g_loss: 0.024801]\n",
            "[Epoch 16: batch 106/265] [d_loss: 0.046435] [g_loss: 0.029978]\n",
            "[Epoch 16: batch 116/265] [d_loss: 0.229914] [g_loss: 0.024505]\n",
            "[Epoch 16: batch 126/265] [d_loss: 0.443476] [g_loss: 0.034649]\n",
            "[Epoch 16: batch 136/265] [d_loss: 0.142037] [g_loss: 0.027058]\n",
            "[Epoch 16: batch 146/265] [d_loss: 0.056765] [g_loss: 0.031181]\n",
            "[Epoch 16: batch 156/265] [d_loss: 0.010679] [g_loss: 0.029199]\n",
            "[Epoch 16: batch 166/265] [d_loss: 0.016221] [g_loss: 0.032470]\n",
            "[Epoch 16: batch 176/265] [d_loss: 0.156863] [g_loss: 0.029328]\n",
            "[Epoch 16: batch 186/265] [d_loss: 0.232829] [g_loss: 0.028909]\n",
            "[Epoch 16: batch 196/265] [d_loss: 0.032985] [g_loss: 0.028616]\n",
            "[Epoch 16: batch 206/265] [d_loss: 0.016783] [g_loss: 0.027538]\n",
            "[Epoch 16: batch 216/265] [d_loss: 0.013550] [g_loss: 0.024160]\n",
            "[Epoch 16: batch 226/265] [d_loss: 0.054366] [g_loss: 0.023407]\n",
            "[Epoch 16: batch 236/265] [d_loss: 0.220269] [g_loss: 0.032254]\n",
            "[Epoch 16: batch 246/265] [d_loss: 0.007474] [g_loss: 0.029836]\n",
            "[Epoch 16: batch 256/265] [d_loss: 0.039066] [g_loss: 0.029563]\n",
            "[Epoch 17: batch 2/265] [d_loss: 0.241567] [g_loss: 0.025488]\n",
            "[Epoch 17: batch 12/265] [d_loss: 0.124293] [g_loss: 0.027349]\n",
            "[Epoch 17: batch 22/265] [d_loss: 0.021891] [g_loss: 0.038550]\n",
            "[Epoch 17: batch 32/265] [d_loss: 0.176955] [g_loss: 0.032986]\n",
            "[Epoch 17: batch 42/265] [d_loss: 0.005456] [g_loss: 0.027402]\n",
            "[Epoch 17: batch 52/265] [d_loss: 0.119763] [g_loss: 0.030832]\n",
            "[Epoch 17: batch 62/265] [d_loss: 0.070702] [g_loss: 0.032469]\n",
            "[Epoch 17: batch 72/265] [d_loss: 0.006149] [g_loss: 0.028772]\n",
            "[Epoch 17: batch 82/265] [d_loss: 0.108233] [g_loss: 0.031518]\n",
            "[Epoch 17: batch 92/265] [d_loss: 0.012915] [g_loss: 0.030948]\n",
            "[Epoch 17: batch 102/265] [d_loss: 0.018371] [g_loss: 0.026983]\n",
            "[Epoch 17: batch 112/265] [d_loss: 0.047562] [g_loss: 0.034028]\n",
            "[Epoch 17: batch 122/265] [d_loss: 0.007354] [g_loss: 0.025903]\n",
            "[Epoch 17: batch 132/265] [d_loss: 0.043563] [g_loss: 0.034562]\n",
            "[Epoch 17: batch 142/265] [d_loss: 0.003683] [g_loss: 0.025785]\n",
            "[Epoch 17: batch 152/265] [d_loss: 0.003693] [g_loss: 0.030163]\n",
            "[Epoch 17: batch 162/265] [d_loss: 0.059647] [g_loss: 0.023904]\n",
            "[Epoch 17: batch 172/265] [d_loss: 0.031742] [g_loss: 0.034955]\n",
            "[Epoch 17: batch 182/265] [d_loss: 0.003195] [g_loss: 0.026442]\n",
            "[Epoch 17: batch 192/265] [d_loss: 0.004094] [g_loss: 0.031098]\n",
            "[Epoch 17: batch 202/265] [d_loss: 0.006663] [g_loss: 0.029496]\n",
            "[Epoch 17: batch 212/265] [d_loss: 0.002197] [g_loss: 0.026315]\n",
            "[Epoch 17: batch 222/265] [d_loss: 0.000867] [g_loss: 0.026724]\n",
            "[Epoch 17: batch 232/265] [d_loss: 0.428669] [g_loss: 0.027321]\n",
            "[Epoch 17: batch 242/265] [d_loss: 0.007417] [g_loss: 0.036066]\n",
            "[Epoch 17: batch 252/265] [d_loss: 0.029103] [g_loss: 0.040297]\n",
            "[Epoch 17: batch 262/265] [d_loss: 0.006236] [g_loss: 0.031691]\n",
            "[Epoch 18: batch 8/265] [d_loss: 0.002359] [g_loss: 0.034927]\n",
            "[Epoch 18: batch 18/265] [d_loss: 0.006690] [g_loss: 0.027163]\n",
            "[Epoch 18: batch 28/265] [d_loss: 0.065542] [g_loss: 0.027030]\n",
            "[Epoch 18: batch 38/265] [d_loss: 0.028249] [g_loss: 0.032887]\n",
            "[Epoch 18: batch 48/265] [d_loss: 0.051077] [g_loss: 0.032289]\n",
            "[Epoch 18: batch 58/265] [d_loss: 0.065831] [g_loss: 0.024082]\n",
            "[Epoch 18: batch 68/265] [d_loss: 0.005919] [g_loss: 0.025046]\n",
            "[Epoch 18: batch 78/265] [d_loss: 0.016517] [g_loss: 0.024507]\n",
            "[Epoch 18: batch 88/265] [d_loss: 0.008658] [g_loss: 0.032441]\n",
            "[Epoch 18: batch 98/265] [d_loss: 0.015023] [g_loss: 0.031377]\n",
            "[Epoch 18: batch 108/265] [d_loss: 0.004784] [g_loss: 0.028441]\n",
            "[Epoch 18: batch 118/265] [d_loss: 0.093523] [g_loss: 0.025818]\n",
            "[Epoch 18: batch 128/265] [d_loss: 0.485621] [g_loss: 0.030138]\n",
            "[Epoch 18: batch 138/265] [d_loss: 0.007783] [g_loss: 0.030263]\n",
            "[Epoch 18: batch 148/265] [d_loss: 0.106855] [g_loss: 0.035952]\n",
            "[Epoch 18: batch 158/265] [d_loss: 0.119507] [g_loss: 0.035144]\n",
            "[Epoch 18: batch 168/265] [d_loss: 0.487816] [g_loss: 0.025900]\n",
            "[Epoch 18: batch 178/265] [d_loss: 0.383209] [g_loss: 0.031898]\n",
            "[Epoch 18: batch 188/265] [d_loss: 0.300922] [g_loss: 0.030857]\n",
            "[Epoch 18: batch 198/265] [d_loss: 0.267601] [g_loss: 0.030249]\n",
            "[Epoch 18: batch 208/265] [d_loss: 0.029319] [g_loss: 0.037109]\n",
            "[Epoch 18: batch 218/265] [d_loss: 0.495031] [g_loss: 0.037772]\n",
            "[Epoch 18: batch 228/265] [d_loss: 0.034715] [g_loss: 0.028479]\n",
            "[Epoch 18: batch 238/265] [d_loss: 0.016197] [g_loss: 0.028569]\n",
            "[Epoch 18: batch 248/265] [d_loss: 0.655738] [g_loss: 0.021628]\n",
            "[Epoch 18: batch 258/265] [d_loss: 0.381323] [g_loss: 0.032294]\n",
            "[Epoch 19: batch 4/265] [d_loss: 0.213021] [g_loss: 0.037938]\n",
            "[Epoch 19: batch 14/265] [d_loss: 0.434976] [g_loss: 0.027267]\n",
            "[Epoch 19: batch 24/265] [d_loss: 0.023869] [g_loss: 0.031283]\n",
            "[Epoch 19: batch 34/265] [d_loss: 0.447284] [g_loss: 0.025794]\n",
            "[Epoch 19: batch 44/265] [d_loss: 0.090872] [g_loss: 0.034526]\n",
            "[Epoch 19: batch 54/265] [d_loss: 0.149369] [g_loss: 0.027277]\n",
            "[Epoch 19: batch 64/265] [d_loss: 0.127104] [g_loss: 0.034678]\n",
            "[Epoch 19: batch 74/265] [d_loss: 0.101026] [g_loss: 0.035792]\n",
            "[Epoch 19: batch 84/265] [d_loss: 0.256896] [g_loss: 0.027840]\n",
            "[Epoch 19: batch 94/265] [d_loss: 0.019587] [g_loss: 0.030068]\n",
            "[Epoch 19: batch 104/265] [d_loss: 0.383736] [g_loss: 0.027421]\n",
            "[Epoch 19: batch 114/265] [d_loss: 0.097986] [g_loss: 0.024924]\n",
            "[Epoch 19: batch 124/265] [d_loss: 0.370384] [g_loss: 0.034481]\n",
            "[Epoch 19: batch 134/265] [d_loss: 0.183464] [g_loss: 0.029981]\n",
            "[Epoch 19: batch 144/265] [d_loss: 0.091052] [g_loss: 0.026362]\n",
            "[Epoch 19: batch 154/265] [d_loss: 0.159799] [g_loss: 0.031398]\n",
            "[Epoch 19: batch 164/265] [d_loss: 0.068223] [g_loss: 0.028096]\n",
            "[Epoch 19: batch 174/265] [d_loss: 0.072800] [g_loss: 0.035438]\n",
            "[Epoch 19: batch 184/265] [d_loss: 0.018647] [g_loss: 0.038502]\n",
            "[Epoch 19: batch 194/265] [d_loss: 0.325940] [g_loss: 0.033931]\n",
            "[Epoch 19: batch 204/265] [d_loss: 0.007719] [g_loss: 0.040444]\n",
            "[Epoch 19: batch 214/265] [d_loss: 0.003949] [g_loss: 0.029377]\n",
            "[Epoch 19: batch 224/265] [d_loss: 0.030208] [g_loss: 0.028947]\n",
            "[Epoch 19: batch 234/265] [d_loss: 0.054479] [g_loss: 0.030031]\n",
            "[Epoch 19: batch 244/265] [d_loss: 0.105648] [g_loss: 0.030153]\n",
            "[Epoch 19: batch 254/265] [d_loss: 0.272553] [g_loss: 0.031198]\n",
            "[Epoch 19: batch 264/265] [d_loss: 0.136205] [g_loss: 0.041355]\n",
            "\n",
            "Saved trained model in /content/drive/MyDrive/SRDRM/checkpoints/USR_4x/srdrm-gan\n",
            "\n",
            "[Epoch 20: batch 10/265] [d_loss: 0.437175] [g_loss: 0.039880]\n",
            "[Epoch 20: batch 20/265] [d_loss: 0.035398] [g_loss: 0.030431]\n",
            "[Epoch 20: batch 30/265] [d_loss: 0.028114] [g_loss: 0.030227]\n",
            "[Epoch 20: batch 40/265] [d_loss: 0.425121] [g_loss: 0.029550]\n",
            "[Epoch 20: batch 50/265] [d_loss: 0.030992] [g_loss: 0.034292]\n",
            "[Epoch 20: batch 60/265] [d_loss: 0.094731] [g_loss: 0.030943]\n",
            "[Epoch 20: batch 70/265] [d_loss: 0.002744] [g_loss: 0.037034]\n",
            "[Epoch 20: batch 80/265] [d_loss: 0.093556] [g_loss: 0.029599]\n",
            "[Epoch 20: batch 90/265] [d_loss: 0.158197] [g_loss: 0.027766]\n",
            "[Epoch 20: batch 100/265] [d_loss: 0.291219] [g_loss: 0.031148]\n",
            "[Epoch 20: batch 110/265] [d_loss: 0.004140] [g_loss: 0.029527]\n",
            "[Epoch 20: batch 120/265] [d_loss: 0.002701] [g_loss: 0.032743]\n",
            "[Epoch 20: batch 130/265] [d_loss: 0.019206] [g_loss: 0.034538]\n",
            "[Epoch 20: batch 140/265] [d_loss: 0.476159] [g_loss: 0.031929]\n",
            "[Epoch 20: batch 150/265] [d_loss: 0.191552] [g_loss: 0.038354]\n",
            "[Epoch 20: batch 160/265] [d_loss: 0.256817] [g_loss: 0.026807]\n",
            "[Epoch 20: batch 170/265] [d_loss: 0.168824] [g_loss: 0.029331]\n",
            "[Epoch 20: batch 180/265] [d_loss: 0.022856] [g_loss: 0.031754]\n",
            "[Epoch 20: batch 190/265] [d_loss: 0.130894] [g_loss: 0.029531]\n",
            "[Epoch 20: batch 200/265] [d_loss: 0.008591] [g_loss: 0.031562]\n",
            "[Epoch 20: batch 210/265] [d_loss: 0.059648] [g_loss: 0.034796]\n",
            "[Epoch 20: batch 220/265] [d_loss: 0.105769] [g_loss: 0.030070]\n",
            "[Epoch 20: batch 230/265] [d_loss: 0.401201] [g_loss: 0.027327]\n",
            "[Epoch 20: batch 240/265] [d_loss: 0.405602] [g_loss: 0.034744]\n",
            "[Epoch 20: batch 250/265] [d_loss: 0.017765] [g_loss: 0.036283]\n",
            "[Epoch 20: batch 260/265] [d_loss: 0.247307] [g_loss: 0.039083]\n",
            "[Epoch 21: batch 6/265] [d_loss: 0.011373] [g_loss: 0.031009]\n",
            "[Epoch 21: batch 16/265] [d_loss: 0.074657] [g_loss: 0.034619]\n",
            "[Epoch 21: batch 26/265] [d_loss: 0.060884] [g_loss: 0.040978]\n",
            "[Epoch 21: batch 36/265] [d_loss: 0.024512] [g_loss: 0.025638]\n",
            "[Epoch 21: batch 46/265] [d_loss: 0.047517] [g_loss: 0.033319]\n",
            "[Epoch 21: batch 56/265] [d_loss: 0.251384] [g_loss: 0.025813]\n",
            "[Epoch 21: batch 66/265] [d_loss: 0.685364] [g_loss: 0.029955]\n",
            "[Epoch 21: batch 76/265] [d_loss: 0.111761] [g_loss: 0.032856]\n",
            "[Epoch 21: batch 86/265] [d_loss: 0.187229] [g_loss: 0.038972]\n",
            "[Epoch 21: batch 96/265] [d_loss: 0.031176] [g_loss: 0.024766]\n",
            "[Epoch 21: batch 106/265] [d_loss: 0.151683] [g_loss: 0.030998]\n",
            "[Epoch 21: batch 116/265] [d_loss: 0.396499] [g_loss: 0.024454]\n",
            "[Epoch 21: batch 126/265] [d_loss: 0.029050] [g_loss: 0.031411]\n",
            "[Epoch 21: batch 136/265] [d_loss: 0.007586] [g_loss: 0.028861]\n",
            "[Epoch 21: batch 146/265] [d_loss: 0.005862] [g_loss: 0.032061]\n",
            "[Epoch 21: batch 156/265] [d_loss: 0.003709] [g_loss: 0.027189]\n",
            "[Epoch 21: batch 166/265] [d_loss: 0.016226] [g_loss: 0.033966]\n",
            "[Epoch 21: batch 176/265] [d_loss: 0.045417] [g_loss: 0.027579]\n",
            "[Epoch 21: batch 186/265] [d_loss: 0.100709] [g_loss: 0.029398]\n",
            "[Epoch 21: batch 196/265] [d_loss: 0.010114] [g_loss: 0.027941]\n",
            "[Epoch 21: batch 206/265] [d_loss: 0.090173] [g_loss: 0.029405]\n",
            "[Epoch 21: batch 216/265] [d_loss: 0.028889] [g_loss: 0.025194]\n",
            "[Epoch 21: batch 226/265] [d_loss: 0.092846] [g_loss: 0.027251]\n",
            "[Epoch 21: batch 236/265] [d_loss: 0.127923] [g_loss: 0.033419]\n",
            "[Epoch 21: batch 246/265] [d_loss: 0.093597] [g_loss: 0.027140]\n",
            "[Epoch 21: batch 256/265] [d_loss: 0.335062] [g_loss: 0.028168]\n",
            "[Epoch 22: batch 2/265] [d_loss: 0.072858] [g_loss: 0.024632]\n",
            "[Epoch 22: batch 12/265] [d_loss: 0.101878] [g_loss: 0.028109]\n",
            "[Epoch 22: batch 22/265] [d_loss: 0.072352] [g_loss: 0.037552]\n",
            "[Epoch 22: batch 32/265] [d_loss: 0.043892] [g_loss: 0.033345]\n",
            "[Epoch 22: batch 42/265] [d_loss: 0.015513] [g_loss: 0.028076]\n",
            "[Epoch 22: batch 52/265] [d_loss: 0.015594] [g_loss: 0.031567]\n",
            "[Epoch 22: batch 62/265] [d_loss: 0.027602] [g_loss: 0.031763]\n",
            "[Epoch 22: batch 72/265] [d_loss: 0.040212] [g_loss: 0.027683]\n",
            "[Epoch 22: batch 82/265] [d_loss: 0.577052] [g_loss: 0.028178]\n",
            "[Epoch 22: batch 92/265] [d_loss: 0.052871] [g_loss: 0.028337]\n",
            "[Epoch 22: batch 102/265] [d_loss: 0.068480] [g_loss: 0.026358]\n",
            "[Epoch 22: batch 112/265] [d_loss: 0.032895] [g_loss: 0.033923]\n",
            "[Epoch 22: batch 122/265] [d_loss: 0.008977] [g_loss: 0.025527]\n",
            "[Epoch 22: batch 132/265] [d_loss: 0.239782] [g_loss: 0.033811]\n",
            "[Epoch 22: batch 142/265] [d_loss: 0.248403] [g_loss: 0.025997]\n",
            "[Epoch 22: batch 152/265] [d_loss: 0.021426] [g_loss: 0.030479]\n",
            "[Epoch 22: batch 162/265] [d_loss: 0.030620] [g_loss: 0.025110]\n",
            "[Epoch 22: batch 172/265] [d_loss: 0.476661] [g_loss: 0.030837]\n",
            "[Epoch 22: batch 182/265] [d_loss: 0.001684] [g_loss: 0.027247]\n",
            "[Epoch 22: batch 192/265] [d_loss: 0.013787] [g_loss: 0.030660]\n",
            "[Epoch 22: batch 202/265] [d_loss: 0.384907] [g_loss: 0.030121]\n",
            "[Epoch 22: batch 212/265] [d_loss: 0.216363] [g_loss: 0.024176]\n",
            "[Epoch 22: batch 222/265] [d_loss: 0.298330] [g_loss: 0.025211]\n",
            "[Epoch 22: batch 232/265] [d_loss: 0.724286] [g_loss: 0.027618]\n",
            "[Epoch 22: batch 242/265] [d_loss: 0.057611] [g_loss: 0.035440]\n",
            "[Epoch 22: batch 252/265] [d_loss: 0.034192] [g_loss: 0.037918]\n",
            "[Epoch 22: batch 262/265] [d_loss: 0.041685] [g_loss: 0.030407]\n",
            "[Epoch 23: batch 8/265] [d_loss: 0.134815] [g_loss: 0.032498]\n",
            "[Epoch 23: batch 18/265] [d_loss: 0.176838] [g_loss: 0.025050]\n",
            "[Epoch 23: batch 28/265] [d_loss: 0.074227] [g_loss: 0.026270]\n",
            "[Epoch 23: batch 38/265] [d_loss: 0.318018] [g_loss: 0.031334]\n",
            "[Epoch 23: batch 48/265] [d_loss: 0.009121] [g_loss: 0.033807]\n",
            "[Epoch 23: batch 58/265] [d_loss: 0.066698] [g_loss: 0.023128]\n",
            "[Epoch 23: batch 68/265] [d_loss: 0.012631] [g_loss: 0.025168]\n",
            "[Epoch 23: batch 78/265] [d_loss: 0.006644] [g_loss: 0.024582]\n",
            "[Epoch 23: batch 88/265] [d_loss: 0.065899] [g_loss: 0.029006]\n",
            "[Epoch 23: batch 98/265] [d_loss: 0.020491] [g_loss: 0.031666]\n",
            "[Epoch 23: batch 108/265] [d_loss: 0.426671] [g_loss: 0.030167]\n",
            "[Epoch 23: batch 118/265] [d_loss: 0.224067] [g_loss: 0.025112]\n",
            "[Epoch 23: batch 128/265] [d_loss: 0.154755] [g_loss: 0.027344]\n",
            "[Epoch 23: batch 138/265] [d_loss: 0.045206] [g_loss: 0.028351]\n",
            "[Epoch 23: batch 148/265] [d_loss: 0.019803] [g_loss: 0.033793]\n",
            "[Epoch 23: batch 158/265] [d_loss: 0.168218] [g_loss: 0.031547]\n",
            "[Epoch 23: batch 168/265] [d_loss: 0.473028] [g_loss: 0.026550]\n",
            "[Epoch 23: batch 178/265] [d_loss: 0.369598] [g_loss: 0.032275]\n",
            "[Epoch 23: batch 188/265] [d_loss: 0.129681] [g_loss: 0.028317]\n",
            "[Epoch 23: batch 198/265] [d_loss: 0.028774] [g_loss: 0.031242]\n",
            "[Epoch 23: batch 208/265] [d_loss: 0.209714] [g_loss: 0.034809]\n",
            "[Epoch 23: batch 218/265] [d_loss: 0.396660] [g_loss: 0.030732]\n",
            "[Epoch 23: batch 228/265] [d_loss: 0.115193] [g_loss: 0.031939]\n",
            "[Epoch 23: batch 238/265] [d_loss: 0.026499] [g_loss: 0.026568]\n",
            "[Epoch 23: batch 248/265] [d_loss: 0.494872] [g_loss: 0.024047]\n",
            "[Epoch 23: batch 258/265] [d_loss: 0.036046] [g_loss: 0.030349]\n",
            "\n",
            "Saved trained model in /content/drive/MyDrive/SRDRM/checkpoints/USR_4x/srdrm-gan\n",
            "\n",
            "[Epoch 24: batch 4/265] [d_loss: 0.211712] [g_loss: 0.034169]\n",
            "[Epoch 24: batch 14/265] [d_loss: 0.458149] [g_loss: 0.028710]\n",
            "[Epoch 24: batch 24/265] [d_loss: 0.023407] [g_loss: 0.031754]\n",
            "[Epoch 24: batch 34/265] [d_loss: 0.417607] [g_loss: 0.025619]\n",
            "[Epoch 24: batch 44/265] [d_loss: 0.079741] [g_loss: 0.034789]\n",
            "[Epoch 24: batch 54/265] [d_loss: 0.456281] [g_loss: 0.027993]\n",
            "[Epoch 24: batch 64/265] [d_loss: 0.014178] [g_loss: 0.032771]\n",
            "[Epoch 24: batch 74/265] [d_loss: 0.055737] [g_loss: 0.035435]\n",
            "[Epoch 24: batch 84/265] [d_loss: 0.108242] [g_loss: 0.028819]\n",
            "[Epoch 24: batch 94/265] [d_loss: 0.004790] [g_loss: 0.028810]\n",
            "[Epoch 24: batch 104/265] [d_loss: 0.421466] [g_loss: 0.028508]\n",
            "[Epoch 24: batch 114/265] [d_loss: 0.393215] [g_loss: 0.024446]\n",
            "[Epoch 24: batch 124/265] [d_loss: 0.134656] [g_loss: 0.032921]\n",
            "[Epoch 24: batch 134/265] [d_loss: 0.041301] [g_loss: 0.030521]\n",
            "[Epoch 24: batch 144/265] [d_loss: 0.247912] [g_loss: 0.028778]\n",
            "[Epoch 24: batch 154/265] [d_loss: 0.083699] [g_loss: 0.032266]\n",
            "[Epoch 24: batch 164/265] [d_loss: 0.085412] [g_loss: 0.028386]\n",
            "[Epoch 24: batch 174/265] [d_loss: 0.054975] [g_loss: 0.031706]\n",
            "[Epoch 24: batch 184/265] [d_loss: 0.250464] [g_loss: 0.039618]\n",
            "[Epoch 24: batch 194/265] [d_loss: 0.172389] [g_loss: 0.032003]\n",
            "[Epoch 24: batch 204/265] [d_loss: 0.003502] [g_loss: 0.040925]\n",
            "[Epoch 24: batch 214/265] [d_loss: 0.050036] [g_loss: 0.027445]\n",
            "[Epoch 24: batch 224/265] [d_loss: 0.007152] [g_loss: 0.030355]\n",
            "[Epoch 24: batch 234/265] [d_loss: 0.157053] [g_loss: 0.029547]\n",
            "[Epoch 24: batch 244/265] [d_loss: 0.178558] [g_loss: 0.030343]\n",
            "[Epoch 24: batch 254/265] [d_loss: 0.016050] [g_loss: 0.031476]\n",
            "[Epoch 24: batch 264/265] [d_loss: 0.155605] [g_loss: 0.042377]\n",
            "[Epoch 25: batch 10/265] [d_loss: 0.325069] [g_loss: 0.039176]\n",
            "[Epoch 25: batch 20/265] [d_loss: 0.012457] [g_loss: 0.028651]\n",
            "[Epoch 25: batch 30/265] [d_loss: 0.003495] [g_loss: 0.031290]\n",
            "[Epoch 25: batch 40/265] [d_loss: 0.422184] [g_loss: 0.030032]\n",
            "[Epoch 25: batch 50/265] [d_loss: 0.097032] [g_loss: 0.034029]\n",
            "[Epoch 25: batch 60/265] [d_loss: 0.014343] [g_loss: 0.031858]\n",
            "[Epoch 25: batch 70/265] [d_loss: 0.032132] [g_loss: 0.036650]\n",
            "[Epoch 25: batch 80/265] [d_loss: 0.020737] [g_loss: 0.030206]\n",
            "[Epoch 25: batch 90/265] [d_loss: 0.327658] [g_loss: 0.028610]\n",
            "[Epoch 25: batch 100/265] [d_loss: 0.096160] [g_loss: 0.031219]\n",
            "[Epoch 25: batch 110/265] [d_loss: 0.004949] [g_loss: 0.027693]\n",
            "[Epoch 25: batch 120/265] [d_loss: 0.011809] [g_loss: 0.032117]\n",
            "[Epoch 25: batch 130/265] [d_loss: 0.154412] [g_loss: 0.030626]\n",
            "[Epoch 25: batch 140/265] [d_loss: 0.474183] [g_loss: 0.034385]\n",
            "[Epoch 25: batch 150/265] [d_loss: 0.289323] [g_loss: 0.037748]\n",
            "[Epoch 25: batch 160/265] [d_loss: 0.151439] [g_loss: 0.026397]\n",
            "[Epoch 25: batch 170/265] [d_loss: 0.065399] [g_loss: 0.027518]\n",
            "[Epoch 25: batch 180/265] [d_loss: 0.055745] [g_loss: 0.031368]\n",
            "[Epoch 25: batch 190/265] [d_loss: 0.210301] [g_loss: 0.033481]\n",
            "[Epoch 25: batch 200/265] [d_loss: 0.075420] [g_loss: 0.035564]\n",
            "[Epoch 25: batch 210/265] [d_loss: 0.199297] [g_loss: 0.036678]\n",
            "[Epoch 25: batch 220/265] [d_loss: 0.011226] [g_loss: 0.032772]\n",
            "[Epoch 25: batch 230/265] [d_loss: 0.169489] [g_loss: 0.033077]\n",
            "[Epoch 25: batch 240/265] [d_loss: 0.114179] [g_loss: 0.034556]\n",
            "[Epoch 25: batch 250/265] [d_loss: 0.004919] [g_loss: 0.037053]\n",
            "[Epoch 25: batch 260/265] [d_loss: 0.128010] [g_loss: 0.038323]\n",
            "[Epoch 26: batch 6/265] [d_loss: 0.003263] [g_loss: 0.032343]\n",
            "[Epoch 26: batch 16/265] [d_loss: 0.007416] [g_loss: 0.035089]\n",
            "[Epoch 26: batch 26/265] [d_loss: 0.039938] [g_loss: 0.040645]\n",
            "[Epoch 26: batch 36/265] [d_loss: 0.034374] [g_loss: 0.026355]\n",
            "[Epoch 26: batch 46/265] [d_loss: 0.119839] [g_loss: 0.035265]\n",
            "[Epoch 26: batch 56/265] [d_loss: 0.184175] [g_loss: 0.027300]\n",
            "[Epoch 26: batch 66/265] [d_loss: 0.445148] [g_loss: 0.030900]\n",
            "[Epoch 26: batch 76/265] [d_loss: 0.028351] [g_loss: 0.031752]\n",
            "[Epoch 26: batch 86/265] [d_loss: 0.038598] [g_loss: 0.039372]\n",
            "[Epoch 26: batch 96/265] [d_loss: 0.010607] [g_loss: 0.025437]\n",
            "[Epoch 26: batch 106/265] [d_loss: 0.149617] [g_loss: 0.031618]\n",
            "[Epoch 26: batch 116/265] [d_loss: 0.513965] [g_loss: 0.029815]\n",
            "[Epoch 26: batch 126/265] [d_loss: 0.234587] [g_loss: 0.033776]\n",
            "[Epoch 26: batch 136/265] [d_loss: 0.102113] [g_loss: 0.030862]\n",
            "[Epoch 26: batch 146/265] [d_loss: 0.203413] [g_loss: 0.029735]\n",
            "[Epoch 26: batch 156/265] [d_loss: 0.026191] [g_loss: 0.028818]\n",
            "[Epoch 26: batch 166/265] [d_loss: 0.091228] [g_loss: 0.033276]\n",
            "[Epoch 26: batch 176/265] [d_loss: 0.020115] [g_loss: 0.029763]\n",
            "[Epoch 26: batch 186/265] [d_loss: 0.012619] [g_loss: 0.030913]\n",
            "[Epoch 26: batch 196/265] [d_loss: 0.045070] [g_loss: 0.027982]\n",
            "[Epoch 26: batch 206/265] [d_loss: 0.023891] [g_loss: 0.028337]\n",
            "[Epoch 26: batch 216/265] [d_loss: 0.059158] [g_loss: 0.024896]\n",
            "[Epoch 26: batch 226/265] [d_loss: 0.036055] [g_loss: 0.025350]\n",
            "[Epoch 26: batch 236/265] [d_loss: 0.318047] [g_loss: 0.031208]\n",
            "[Epoch 26: batch 246/265] [d_loss: 0.049629] [g_loss: 0.028802]\n",
            "[Epoch 26: batch 256/265] [d_loss: 0.388272] [g_loss: 0.028831]\n",
            "[Epoch 27: batch 2/265] [d_loss: 0.010262] [g_loss: 0.026994]\n",
            "[Epoch 27: batch 12/265] [d_loss: 0.071243] [g_loss: 0.032013]\n",
            "[Epoch 27: batch 22/265] [d_loss: 0.008404] [g_loss: 0.038450]\n",
            "[Epoch 27: batch 32/265] [d_loss: 0.039263] [g_loss: 0.034373]\n",
            "[Epoch 27: batch 42/265] [d_loss: 0.155005] [g_loss: 0.028347]\n",
            "[Epoch 27: batch 52/265] [d_loss: 0.352283] [g_loss: 0.032381]\n",
            "[Epoch 27: batch 62/265] [d_loss: 0.072679] [g_loss: 0.032545]\n",
            "[Epoch 27: batch 72/265] [d_loss: 0.022915] [g_loss: 0.029604]\n",
            "[Epoch 27: batch 82/265] [d_loss: 0.398650] [g_loss: 0.031986]\n",
            "[Epoch 27: batch 92/265] [d_loss: 0.031769] [g_loss: 0.026473]\n",
            "[Epoch 27: batch 102/265] [d_loss: 0.014076] [g_loss: 0.029660]\n",
            "[Epoch 27: batch 112/265] [d_loss: 0.204972] [g_loss: 0.032446]\n",
            "[Epoch 27: batch 122/265] [d_loss: 0.350043] [g_loss: 0.025567]\n",
            "[Epoch 27: batch 132/265] [d_loss: 0.017856] [g_loss: 0.032113]\n",
            "[Epoch 27: batch 142/265] [d_loss: 0.186018] [g_loss: 0.024039]\n",
            "[Epoch 27: batch 152/265] [d_loss: 0.253167] [g_loss: 0.031064]\n",
            "[Epoch 27: batch 162/265] [d_loss: 0.048582] [g_loss: 0.023147]\n",
            "[Epoch 27: batch 172/265] [d_loss: 0.019727] [g_loss: 0.031118]\n",
            "[Epoch 27: batch 182/265] [d_loss: 0.008138] [g_loss: 0.027035]\n",
            "[Epoch 27: batch 192/265] [d_loss: 0.048589] [g_loss: 0.029940]\n",
            "[Epoch 27: batch 202/265] [d_loss: 0.253720] [g_loss: 0.028737]\n",
            "[Epoch 27: batch 212/265] [d_loss: 0.072859] [g_loss: 0.024238]\n",
            "[Epoch 27: batch 222/265] [d_loss: 0.052700] [g_loss: 0.025105]\n",
            "[Epoch 27: batch 232/265] [d_loss: 0.102234] [g_loss: 0.030097]\n",
            "[Epoch 27: batch 242/265] [d_loss: 0.185956] [g_loss: 0.034833]\n",
            "[Epoch 27: batch 252/265] [d_loss: 0.015103] [g_loss: 0.037573]\n",
            "[Epoch 27: batch 262/265] [d_loss: 0.009239] [g_loss: 0.032016]\n",
            "\n",
            "Saved trained model in /content/drive/MyDrive/SRDRM/checkpoints/USR_4x/srdrm-gan\n",
            "\n",
            "[Epoch 28: batch 8/265] [d_loss: 0.411804] [g_loss: 0.035125]\n",
            "[Epoch 28: batch 18/265] [d_loss: 0.209888] [g_loss: 0.024503]\n",
            "[Epoch 28: batch 28/265] [d_loss: 0.517454] [g_loss: 0.026193]\n",
            "[Epoch 28: batch 38/265] [d_loss: 0.030085] [g_loss: 0.031631]\n",
            "[Epoch 28: batch 48/265] [d_loss: 0.008608] [g_loss: 0.033318]\n",
            "[Epoch 28: batch 58/265] [d_loss: 0.120521] [g_loss: 0.022809]\n",
            "[Epoch 28: batch 68/265] [d_loss: 0.005398] [g_loss: 0.025640]\n",
            "[Epoch 28: batch 78/265] [d_loss: 0.174255] [g_loss: 0.025244]\n",
            "[Epoch 28: batch 88/265] [d_loss: 0.076874] [g_loss: 0.029967]\n",
            "[Epoch 28: batch 98/265] [d_loss: 0.012702] [g_loss: 0.031174]\n",
            "[Epoch 28: batch 108/265] [d_loss: 0.471467] [g_loss: 0.028353]\n",
            "[Epoch 28: batch 118/265] [d_loss: 0.020325] [g_loss: 0.027136]\n",
            "[Epoch 28: batch 128/265] [d_loss: 0.118461] [g_loss: 0.026823]\n",
            "[Epoch 28: batch 138/265] [d_loss: 0.278358] [g_loss: 0.026886]\n",
            "[Epoch 28: batch 148/265] [d_loss: 0.071866] [g_loss: 0.033147]\n",
            "[Epoch 28: batch 158/265] [d_loss: 0.154333] [g_loss: 0.031600]\n",
            "[Epoch 28: batch 168/265] [d_loss: 0.466967] [g_loss: 0.025218]\n",
            "[Epoch 28: batch 178/265] [d_loss: 0.092262] [g_loss: 0.032036]\n",
            "[Epoch 28: batch 188/265] [d_loss: 0.123336] [g_loss: 0.026842]\n",
            "[Epoch 28: batch 198/265] [d_loss: 0.099675] [g_loss: 0.031390]\n",
            "[Epoch 28: batch 208/265] [d_loss: 0.259600] [g_loss: 0.033364]\n",
            "[Epoch 28: batch 218/265] [d_loss: 0.455108] [g_loss: 0.032804]\n",
            "[Epoch 28: batch 228/265] [d_loss: 0.085415] [g_loss: 0.027918]\n",
            "[Epoch 28: batch 238/265] [d_loss: 0.162950] [g_loss: 0.024815]\n",
            "[Epoch 28: batch 248/265] [d_loss: 0.448687] [g_loss: 0.024041]\n",
            "[Epoch 28: batch 258/265] [d_loss: 0.062086] [g_loss: 0.030524]\n",
            "[Epoch 29: batch 4/265] [d_loss: 0.063889] [g_loss: 0.034150]\n",
            "[Epoch 29: batch 14/265] [d_loss: 0.435851] [g_loss: 0.024514]\n",
            "[Epoch 29: batch 24/265] [d_loss: 0.019594] [g_loss: 0.030772]\n",
            "[Epoch 29: batch 34/265] [d_loss: 0.457841] [g_loss: 0.025355]\n",
            "[Epoch 29: batch 44/265] [d_loss: 0.045798] [g_loss: 0.033999]\n",
            "[Epoch 29: batch 54/265] [d_loss: 0.393271] [g_loss: 0.024241]\n",
            "[Epoch 29: batch 64/265] [d_loss: 0.130988] [g_loss: 0.033655]\n",
            "[Epoch 29: batch 74/265] [d_loss: 0.058893] [g_loss: 0.033860]\n",
            "[Epoch 29: batch 84/265] [d_loss: 0.154567] [g_loss: 0.026802]\n",
            "[Epoch 29: batch 94/265] [d_loss: 0.005736] [g_loss: 0.027644]\n",
            "[Epoch 29: batch 104/265] [d_loss: 0.184573] [g_loss: 0.026390]\n",
            "[Epoch 29: batch 114/265] [d_loss: 0.025224] [g_loss: 0.023626]\n",
            "[Epoch 29: batch 124/265] [d_loss: 0.355705] [g_loss: 0.032444]\n",
            "[Epoch 29: batch 134/265] [d_loss: 0.032685] [g_loss: 0.029513]\n",
            "[Epoch 29: batch 144/265] [d_loss: 0.088864] [g_loss: 0.024689]\n",
            "[Epoch 29: batch 154/265] [d_loss: 0.132251] [g_loss: 0.029702]\n",
            "[Epoch 29: batch 164/265] [d_loss: 0.009355] [g_loss: 0.027424]\n",
            "[Epoch 29: batch 174/265] [d_loss: 0.247564] [g_loss: 0.029532]\n",
            "[Epoch 29: batch 184/265] [d_loss: 0.013795] [g_loss: 0.034645]\n",
            "[Epoch 29: batch 194/265] [d_loss: 0.016674] [g_loss: 0.031400]\n",
            "[Epoch 29: batch 204/265] [d_loss: 0.177309] [g_loss: 0.039908]\n",
            "[Epoch 29: batch 214/265] [d_loss: 0.016662] [g_loss: 0.026683]\n",
            "[Epoch 29: batch 224/265] [d_loss: 0.039980] [g_loss: 0.029387]\n",
            "[Epoch 29: batch 234/265] [d_loss: 0.202687] [g_loss: 0.028395]\n",
            "[Epoch 29: batch 244/265] [d_loss: 0.168195] [g_loss: 0.031546]\n",
            "[Epoch 29: batch 254/265] [d_loss: 0.020000] [g_loss: 0.032133]\n",
            "[Epoch 29: batch 264/265] [d_loss: 0.317454] [g_loss: 0.040827]\n",
            "[Epoch 30: batch 10/265] [d_loss: 0.180406] [g_loss: 0.035459]\n",
            "[Epoch 30: batch 20/265] [d_loss: 0.073440] [g_loss: 0.030013]\n",
            "[Epoch 30: batch 30/265] [d_loss: 0.005510] [g_loss: 0.031420]\n",
            "[Epoch 30: batch 40/265] [d_loss: 0.340560] [g_loss: 0.029156]\n",
            "[Epoch 30: batch 50/265] [d_loss: 0.015888] [g_loss: 0.033908]\n",
            "[Epoch 30: batch 60/265] [d_loss: 0.058508] [g_loss: 0.030457]\n",
            "[Epoch 30: batch 70/265] [d_loss: 0.011661] [g_loss: 0.036804]\n",
            "[Epoch 30: batch 80/265] [d_loss: 0.071276] [g_loss: 0.029164]\n",
            "[Epoch 30: batch 90/265] [d_loss: 0.350294] [g_loss: 0.025497]\n",
            "[Epoch 30: batch 100/265] [d_loss: 0.020565] [g_loss: 0.031963]\n",
            "[Epoch 30: batch 110/265] [d_loss: 0.000961] [g_loss: 0.029366]\n",
            "[Epoch 30: batch 120/265] [d_loss: 0.268281] [g_loss: 0.031617]\n",
            "[Epoch 30: batch 130/265] [d_loss: 0.228527] [g_loss: 0.033190]\n",
            "[Epoch 30: batch 140/265] [d_loss: 0.475945] [g_loss: 0.035652]\n",
            "[Epoch 30: batch 150/265] [d_loss: 0.086194] [g_loss: 0.035726]\n",
            "[Epoch 30: batch 160/265] [d_loss: 0.152121] [g_loss: 0.027370]\n",
            "[Epoch 30: batch 170/265] [d_loss: 0.076137] [g_loss: 0.027514]\n",
            "[Epoch 30: batch 180/265] [d_loss: 0.001939] [g_loss: 0.031541]\n",
            "[Epoch 30: batch 190/265] [d_loss: 0.157902] [g_loss: 0.028219]\n",
            "[Epoch 30: batch 200/265] [d_loss: 0.104629] [g_loss: 0.032436]\n",
            "[Epoch 30: batch 210/265] [d_loss: 0.168549] [g_loss: 0.032566]\n",
            "[Epoch 30: batch 220/265] [d_loss: 0.068377] [g_loss: 0.031075]\n",
            "[Epoch 30: batch 230/265] [d_loss: 0.306780] [g_loss: 0.026103]\n",
            "[Epoch 30: batch 240/265] [d_loss: 0.163864] [g_loss: 0.028878]\n",
            "[Epoch 30: batch 250/265] [d_loss: 0.212539] [g_loss: 0.033478]\n",
            "[Epoch 30: batch 260/265] [d_loss: 0.002438] [g_loss: 0.033775]\n",
            "[Epoch 31: batch 6/265] [d_loss: 0.007684] [g_loss: 0.027675]\n",
            "[Epoch 31: batch 16/265] [d_loss: 0.189899] [g_loss: 0.033470]\n",
            "[Epoch 31: batch 26/265] [d_loss: 0.032689] [g_loss: 0.041241]\n",
            "[Epoch 31: batch 36/265] [d_loss: 0.013563] [g_loss: 0.024294]\n",
            "[Epoch 31: batch 46/265] [d_loss: 0.042434] [g_loss: 0.031324]\n",
            "[Epoch 31: batch 56/265] [d_loss: 0.217827] [g_loss: 0.026770]\n",
            "[Epoch 31: batch 66/265] [d_loss: 0.391734] [g_loss: 0.028650]\n",
            "[Epoch 31: batch 76/265] [d_loss: 0.174725] [g_loss: 0.033135]\n",
            "[Epoch 31: batch 86/265] [d_loss: 0.102901] [g_loss: 0.035385]\n",
            "[Epoch 31: batch 96/265] [d_loss: 0.005727] [g_loss: 0.024128]\n",
            "[Epoch 31: batch 106/265] [d_loss: 0.096178] [g_loss: 0.027937]\n",
            "[Epoch 31: batch 116/265] [d_loss: 0.234774] [g_loss: 0.021540]\n",
            "[Epoch 31: batch 126/265] [d_loss: 0.409085] [g_loss: 0.030902]\n",
            "[Epoch 31: batch 136/265] [d_loss: 0.041065] [g_loss: 0.026727]\n",
            "[Epoch 31: batch 146/265] [d_loss: 0.252502] [g_loss: 0.029098]\n",
            "[Epoch 31: batch 156/265] [d_loss: 0.027063] [g_loss: 0.026706]\n",
            "[Epoch 31: batch 166/265] [d_loss: 0.065091] [g_loss: 0.032620]\n",
            "[Epoch 31: batch 176/265] [d_loss: 0.016060] [g_loss: 0.030159]\n",
            "[Epoch 31: batch 186/265] [d_loss: 0.253088] [g_loss: 0.030451]\n",
            "[Epoch 31: batch 196/265] [d_loss: 0.065867] [g_loss: 0.027502]\n",
            "[Epoch 31: batch 206/265] [d_loss: 0.090321] [g_loss: 0.025693]\n",
            "[Epoch 31: batch 216/265] [d_loss: 0.134444] [g_loss: 0.024852]\n",
            "[Epoch 31: batch 226/265] [d_loss: 0.023840] [g_loss: 0.022733]\n",
            "[Epoch 31: batch 236/265] [d_loss: 0.156011] [g_loss: 0.031138]\n",
            "[Epoch 31: batch 246/265] [d_loss: 0.031453] [g_loss: 0.026814]\n",
            "[Epoch 31: batch 256/265] [d_loss: 0.045760] [g_loss: 0.026406]\n",
            "\n",
            "Saved trained model in /content/drive/MyDrive/SRDRM/checkpoints/USR_4x/srdrm-gan\n",
            "\n",
            "[Epoch 32: batch 2/265] [d_loss: 0.169779] [g_loss: 0.025915]\n",
            "[Epoch 32: batch 12/265] [d_loss: 0.359725] [g_loss: 0.027587]\n",
            "[Epoch 32: batch 22/265] [d_loss: 0.012464] [g_loss: 0.037404]\n",
            "[Epoch 32: batch 32/265] [d_loss: 0.191092] [g_loss: 0.034623]\n",
            "[Epoch 32: batch 42/265] [d_loss: 0.019927] [g_loss: 0.027621]\n",
            "[Epoch 32: batch 52/265] [d_loss: 0.145465] [g_loss: 0.030115]\n",
            "[Epoch 32: batch 62/265] [d_loss: 0.016094] [g_loss: 0.032002]\n",
            "[Epoch 32: batch 72/265] [d_loss: 0.027952] [g_loss: 0.026510]\n",
            "[Epoch 32: batch 82/265] [d_loss: 0.378027] [g_loss: 0.032342]\n",
            "[Epoch 32: batch 92/265] [d_loss: 0.012232] [g_loss: 0.027130]\n",
            "[Epoch 32: batch 102/265] [d_loss: 0.033805] [g_loss: 0.026863]\n",
            "[Epoch 32: batch 112/265] [d_loss: 0.139453] [g_loss: 0.032262]\n",
            "[Epoch 32: batch 122/265] [d_loss: 0.219451] [g_loss: 0.024820]\n",
            "[Epoch 32: batch 132/265] [d_loss: 0.036906] [g_loss: 0.032676]\n",
            "[Epoch 32: batch 142/265] [d_loss: 0.145005] [g_loss: 0.024695]\n",
            "[Epoch 32: batch 152/265] [d_loss: 0.054506] [g_loss: 0.032278]\n",
            "[Epoch 32: batch 162/265] [d_loss: 0.007918] [g_loss: 0.023446]\n",
            "[Epoch 32: batch 172/265] [d_loss: 0.005542] [g_loss: 0.031256]\n",
            "[Epoch 32: batch 182/265] [d_loss: 0.015745] [g_loss: 0.025642]\n",
            "[Epoch 32: batch 192/265] [d_loss: 0.133007] [g_loss: 0.029015]\n",
            "[Epoch 32: batch 202/265] [d_loss: 0.240197] [g_loss: 0.028906]\n",
            "[Epoch 32: batch 212/265] [d_loss: 0.013940] [g_loss: 0.024247]\n",
            "[Epoch 32: batch 222/265] [d_loss: 0.030438] [g_loss: 0.025062]\n",
            "[Epoch 32: batch 232/265] [d_loss: 0.048300] [g_loss: 0.028601]\n",
            "[Epoch 32: batch 242/265] [d_loss: 0.006205] [g_loss: 0.034192]\n",
            "[Epoch 32: batch 252/265] [d_loss: 0.035125] [g_loss: 0.036954]\n",
            "[Epoch 32: batch 262/265] [d_loss: 0.044826] [g_loss: 0.029994]\n",
            "[Epoch 33: batch 8/265] [d_loss: 0.417093] [g_loss: 0.033519]\n",
            "[Epoch 33: batch 18/265] [d_loss: 0.001402] [g_loss: 0.027618]\n",
            "[Epoch 33: batch 28/265] [d_loss: 0.474264] [g_loss: 0.027502]\n",
            "[Epoch 33: batch 38/265] [d_loss: 0.016812] [g_loss: 0.030086]\n",
            "[Epoch 33: batch 48/265] [d_loss: 0.219538] [g_loss: 0.029259]\n",
            "[Epoch 33: batch 58/265] [d_loss: 0.100936] [g_loss: 0.019858]\n",
            "[Epoch 33: batch 68/265] [d_loss: 0.096030] [g_loss: 0.023576]\n",
            "[Epoch 33: batch 78/265] [d_loss: 0.095225] [g_loss: 0.026060]\n",
            "[Epoch 33: batch 88/265] [d_loss: 0.028913] [g_loss: 0.028689]\n",
            "[Epoch 33: batch 98/265] [d_loss: 0.022992] [g_loss: 0.031023]\n",
            "[Epoch 33: batch 108/265] [d_loss: 0.019372] [g_loss: 0.026619]\n",
            "[Epoch 33: batch 118/265] [d_loss: 0.176125] [g_loss: 0.024272]\n",
            "[Epoch 33: batch 128/265] [d_loss: 0.357000] [g_loss: 0.027000]\n",
            "[Epoch 33: batch 138/265] [d_loss: 0.337999] [g_loss: 0.027341]\n",
            "[Epoch 33: batch 148/265] [d_loss: 0.036276] [g_loss: 0.034683]\n",
            "[Epoch 33: batch 158/265] [d_loss: 0.026251] [g_loss: 0.033363]\n",
            "[Epoch 33: batch 168/265] [d_loss: 0.468170] [g_loss: 0.025227]\n",
            "[Epoch 33: batch 178/265] [d_loss: 0.417888] [g_loss: 0.034004]\n",
            "[Epoch 33: batch 188/265] [d_loss: 0.008719] [g_loss: 0.028214]\n",
            "[Epoch 33: batch 198/265] [d_loss: 0.042648] [g_loss: 0.028609]\n",
            "[Epoch 33: batch 208/265] [d_loss: 0.037966] [g_loss: 0.033709]\n",
            "[Epoch 33: batch 218/265] [d_loss: 0.427583] [g_loss: 0.031853]\n",
            "[Epoch 33: batch 228/265] [d_loss: 0.037421] [g_loss: 0.029047]\n",
            "[Epoch 33: batch 238/265] [d_loss: 0.020182] [g_loss: 0.025734]\n",
            "[Epoch 33: batch 248/265] [d_loss: 0.445908] [g_loss: 0.023514]\n",
            "[Epoch 33: batch 258/265] [d_loss: 0.010928] [g_loss: 0.028035]\n",
            "[Epoch 34: batch 4/265] [d_loss: 0.241005] [g_loss: 0.033778]\n",
            "[Epoch 34: batch 14/265] [d_loss: 0.331947] [g_loss: 0.023184]\n",
            "[Epoch 34: batch 24/265] [d_loss: 0.060784] [g_loss: 0.029972]\n",
            "[Epoch 34: batch 34/265] [d_loss: 0.441215] [g_loss: 0.023858]\n",
            "[Epoch 34: batch 44/265] [d_loss: 0.024394] [g_loss: 0.034390]\n",
            "[Epoch 34: batch 54/265] [d_loss: 0.366724] [g_loss: 0.025756]\n",
            "[Epoch 34: batch 64/265] [d_loss: 0.227445] [g_loss: 0.032147]\n",
            "[Epoch 34: batch 74/265] [d_loss: 0.005801] [g_loss: 0.036682]\n",
            "[Epoch 34: batch 84/265] [d_loss: 0.027118] [g_loss: 0.029981]\n",
            "[Epoch 34: batch 94/265] [d_loss: 0.058527] [g_loss: 0.027838]\n",
            "[Epoch 34: batch 104/265] [d_loss: 0.353717] [g_loss: 0.026010]\n",
            "[Epoch 34: batch 114/265] [d_loss: 0.068016] [g_loss: 0.024543]\n",
            "[Epoch 34: batch 124/265] [d_loss: 0.340736] [g_loss: 0.032492]\n",
            "[Epoch 34: batch 134/265] [d_loss: 0.017847] [g_loss: 0.029905]\n",
            "[Epoch 34: batch 144/265] [d_loss: 0.086377] [g_loss: 0.023478]\n",
            "[Epoch 34: batch 154/265] [d_loss: 0.016043] [g_loss: 0.029389]\n",
            "[Epoch 34: batch 164/265] [d_loss: 0.003386] [g_loss: 0.026957]\n",
            "[Epoch 34: batch 174/265] [d_loss: 0.016986] [g_loss: 0.030752]\n",
            "[Epoch 34: batch 184/265] [d_loss: 0.002727] [g_loss: 0.036719]\n",
            "[Epoch 34: batch 194/265] [d_loss: 0.013914] [g_loss: 0.033013]\n",
            "[Epoch 34: batch 204/265] [d_loss: 0.002037] [g_loss: 0.040683]\n",
            "[Epoch 34: batch 214/265] [d_loss: 0.105486] [g_loss: 0.026021]\n",
            "[Epoch 34: batch 224/265] [d_loss: 0.001512] [g_loss: 0.030584]\n",
            "[Epoch 34: batch 234/265] [d_loss: 0.302947] [g_loss: 0.027796]\n",
            "[Epoch 34: batch 244/265] [d_loss: 0.003218] [g_loss: 0.029655]\n",
            "[Epoch 34: batch 254/265] [d_loss: 0.005372] [g_loss: 0.031838]\n",
            "[Epoch 34: batch 264/265] [d_loss: 0.096140] [g_loss: 0.041943]\n",
            "[Epoch 35: batch 10/265] [d_loss: 0.009423] [g_loss: 0.035683]\n",
            "[Epoch 35: batch 20/265] [d_loss: 0.033682] [g_loss: 0.028632]\n",
            "[Epoch 35: batch 30/265] [d_loss: 0.007210] [g_loss: 0.031147]\n",
            "[Epoch 35: batch 40/265] [d_loss: 0.111114] [g_loss: 0.027244]\n",
            "[Epoch 35: batch 50/265] [d_loss: 0.109130] [g_loss: 0.034367]\n",
            "[Epoch 35: batch 60/265] [d_loss: 0.025387] [g_loss: 0.030362]\n",
            "[Epoch 35: batch 70/265] [d_loss: 0.002182] [g_loss: 0.036500]\n",
            "[Epoch 35: batch 80/265] [d_loss: 0.009923] [g_loss: 0.030061]\n",
            "[Epoch 35: batch 90/265] [d_loss: 0.036534] [g_loss: 0.027594]\n",
            "[Epoch 35: batch 100/265] [d_loss: 0.004385] [g_loss: 0.032170]\n",
            "[Epoch 35: batch 110/265] [d_loss: 0.000764] [g_loss: 0.028270]\n",
            "[Epoch 35: batch 120/265] [d_loss: 0.001263] [g_loss: 0.031006]\n",
            "[Epoch 35: batch 130/265] [d_loss: 0.002328] [g_loss: 0.030993]\n",
            "[Epoch 35: batch 140/265] [d_loss: 0.192870] [g_loss: 0.029808]\n",
            "[Epoch 35: batch 150/265] [d_loss: 0.010349] [g_loss: 0.037251]\n",
            "[Epoch 35: batch 160/265] [d_loss: 0.005042] [g_loss: 0.027188]\n",
            "[Epoch 35: batch 170/265] [d_loss: 0.000460] [g_loss: 0.028282]\n",
            "[Epoch 35: batch 180/265] [d_loss: 0.002753] [g_loss: 0.031323]\n",
            "[Epoch 35: batch 190/265] [d_loss: 0.007945] [g_loss: 0.031160]\n",
            "[Epoch 35: batch 200/265] [d_loss: 0.004070] [g_loss: 0.032229]\n",
            "[Epoch 35: batch 210/265] [d_loss: 0.002070] [g_loss: 0.034125]\n",
            "[Epoch 35: batch 220/265] [d_loss: 0.027734] [g_loss: 0.030412]\n",
            "[Epoch 35: batch 230/265] [d_loss: 0.223037] [g_loss: 0.025810]\n",
            "[Epoch 35: batch 240/265] [d_loss: 0.002362] [g_loss: 0.031976]\n",
            "[Epoch 35: batch 250/265] [d_loss: 0.011084] [g_loss: 0.036130]\n",
            "[Epoch 35: batch 260/265] [d_loss: 0.002771] [g_loss: 0.033309]\n",
            "\n",
            "Saved trained model in /content/drive/MyDrive/SRDRM/checkpoints/USR_4x/srdrm-gan\n",
            "\n",
            "[Epoch 36: batch 6/265] [d_loss: 0.006830] [g_loss: 0.031590]\n",
            "[Epoch 36: batch 16/265] [d_loss: 0.060658] [g_loss: 0.032363]\n",
            "[Epoch 36: batch 26/265] [d_loss: 0.000245] [g_loss: 0.043096]\n",
            "[Epoch 36: batch 36/265] [d_loss: 0.000724] [g_loss: 0.026379]\n",
            "[Epoch 36: batch 46/265] [d_loss: 0.001794] [g_loss: 0.032852]\n",
            "[Epoch 36: batch 56/265] [d_loss: 0.001100] [g_loss: 0.027868]\n",
            "[Epoch 36: batch 66/265] [d_loss: 0.001973] [g_loss: 0.030574]\n",
            "[Epoch 36: batch 76/265] [d_loss: 0.008543] [g_loss: 0.029239]\n",
            "[Epoch 36: batch 86/265] [d_loss: 0.009926] [g_loss: 0.037954]\n",
            "[Epoch 36: batch 96/265] [d_loss: 0.033746] [g_loss: 0.024189]\n",
            "[Epoch 36: batch 106/265] [d_loss: 0.017841] [g_loss: 0.029994]\n",
            "[Epoch 36: batch 116/265] [d_loss: 0.024363] [g_loss: 0.026914]\n",
            "[Epoch 36: batch 126/265] [d_loss: 0.002843] [g_loss: 0.032805]\n",
            "[Epoch 36: batch 136/265] [d_loss: 0.013730] [g_loss: 0.028536]\n",
            "[Epoch 36: batch 146/265] [d_loss: 0.003460] [g_loss: 0.032769]\n",
            "[Epoch 36: batch 156/265] [d_loss: 0.000442] [g_loss: 0.029129]\n",
            "[Epoch 36: batch 166/265] [d_loss: 0.000128] [g_loss: 0.035042]\n",
            "[Epoch 36: batch 176/265] [d_loss: 0.025640] [g_loss: 0.030757]\n",
            "[Epoch 36: batch 186/265] [d_loss: 0.072172] [g_loss: 0.030366]\n",
            "[Epoch 36: batch 196/265] [d_loss: 0.008971] [g_loss: 0.028170]\n",
            "[Epoch 36: batch 206/265] [d_loss: 0.002598] [g_loss: 0.030428]\n",
            "[Epoch 36: batch 216/265] [d_loss: 0.004017] [g_loss: 0.025143]\n",
            "[Epoch 36: batch 226/265] [d_loss: 0.012904] [g_loss: 0.023239]\n",
            "[Epoch 36: batch 236/265] [d_loss: 0.496975] [g_loss: 0.036321]\n",
            "[Epoch 36: batch 246/265] [d_loss: 0.158408] [g_loss: 0.029695]\n",
            "[Epoch 36: batch 256/265] [d_loss: 0.003714] [g_loss: 0.034232]\n",
            "[Epoch 37: batch 2/265] [d_loss: 0.000597] [g_loss: 0.027943]\n",
            "[Epoch 37: batch 12/265] [d_loss: 0.158371] [g_loss: 0.026746]\n",
            "[Epoch 37: batch 22/265] [d_loss: 0.028658] [g_loss: 0.037045]\n",
            "[Epoch 37: batch 32/265] [d_loss: 0.000069] [g_loss: 0.034875]\n",
            "[Epoch 37: batch 42/265] [d_loss: 0.143899] [g_loss: 0.029728]\n",
            "[Epoch 37: batch 52/265] [d_loss: 0.089899] [g_loss: 0.032312]\n",
            "[Epoch 37: batch 62/265] [d_loss: 0.003530] [g_loss: 0.033456]\n",
            "[Epoch 37: batch 72/265] [d_loss: 0.006191] [g_loss: 0.031536]\n",
            "[Epoch 37: batch 82/265] [d_loss: 0.274077] [g_loss: 0.032197]\n",
            "[Epoch 37: batch 92/265] [d_loss: 0.007813] [g_loss: 0.029906]\n",
            "[Epoch 37: batch 102/265] [d_loss: 0.000414] [g_loss: 0.028354]\n",
            "[Epoch 37: batch 112/265] [d_loss: 0.006895] [g_loss: 0.034597]\n",
            "[Epoch 37: batch 122/265] [d_loss: 0.011492] [g_loss: 0.025961]\n",
            "[Epoch 37: batch 132/265] [d_loss: 0.003007] [g_loss: 0.033901]\n",
            "[Epoch 37: batch 142/265] [d_loss: 0.007045] [g_loss: 0.026877]\n",
            "[Epoch 37: batch 152/265] [d_loss: 0.008310] [g_loss: 0.029167]\n",
            "[Epoch 37: batch 162/265] [d_loss: 0.000005] [g_loss: 0.027394]\n",
            "[Epoch 37: batch 172/265] [d_loss: 0.000097] [g_loss: 0.034630]\n",
            "[Epoch 37: batch 182/265] [d_loss: 0.001001] [g_loss: 0.026587]\n",
            "[Epoch 37: batch 192/265] [d_loss: 0.002421] [g_loss: 0.029653]\n",
            "[Epoch 37: batch 202/265] [d_loss: 0.016043] [g_loss: 0.027365]\n",
            "[Epoch 37: batch 212/265] [d_loss: 0.004383] [g_loss: 0.027894]\n",
            "[Epoch 37: batch 222/265] [d_loss: 0.000322] [g_loss: 0.026463]\n",
            "[Epoch 37: batch 232/265] [d_loss: 0.004323] [g_loss: 0.029947]\n",
            "[Epoch 37: batch 242/265] [d_loss: 0.005653] [g_loss: 0.035638]\n",
            "[Epoch 37: batch 252/265] [d_loss: 0.017793] [g_loss: 0.038520]\n",
            "[Epoch 37: batch 262/265] [d_loss: 0.019001] [g_loss: 0.031028]\n",
            "[Epoch 38: batch 8/265] [d_loss: 0.213069] [g_loss: 0.032822]\n",
            "[Epoch 38: batch 18/265] [d_loss: 0.072893] [g_loss: 0.027920]\n",
            "[Epoch 38: batch 28/265] [d_loss: 0.006209] [g_loss: 0.026780]\n",
            "[Epoch 38: batch 38/265] [d_loss: 0.248964] [g_loss: 0.035165]\n",
            "[Epoch 38: batch 48/265] [d_loss: 0.001300] [g_loss: 0.030276]\n",
            "[Epoch 38: batch 58/265] [d_loss: 0.006644] [g_loss: 0.023430]\n",
            "[Epoch 38: batch 68/265] [d_loss: 0.208793] [g_loss: 0.029197]\n",
            "[Epoch 38: batch 78/265] [d_loss: 0.187522] [g_loss: 0.025989]\n",
            "[Epoch 38: batch 88/265] [d_loss: 0.008162] [g_loss: 0.031281]\n",
            "[Epoch 38: batch 98/265] [d_loss: 0.007862] [g_loss: 0.029115]\n",
            "[Epoch 38: batch 108/265] [d_loss: 0.059363] [g_loss: 0.029504]\n",
            "[Epoch 38: batch 118/265] [d_loss: 0.269238] [g_loss: 0.023171]\n",
            "[Epoch 38: batch 128/265] [d_loss: 0.082277] [g_loss: 0.027365]\n",
            "[Epoch 38: batch 138/265] [d_loss: 0.006308] [g_loss: 0.026390]\n",
            "[Epoch 38: batch 148/265] [d_loss: 0.005899] [g_loss: 0.034892]\n",
            "[Epoch 38: batch 158/265] [d_loss: 0.015844] [g_loss: 0.033008]\n",
            "[Epoch 38: batch 168/265] [d_loss: 0.503669] [g_loss: 0.026472]\n",
            "[Epoch 38: batch 178/265] [d_loss: 0.277087] [g_loss: 0.036067]\n",
            "[Epoch 38: batch 188/265] [d_loss: 0.000820] [g_loss: 0.028403]\n",
            "[Epoch 38: batch 198/265] [d_loss: 0.001482] [g_loss: 0.029779]\n",
            "[Epoch 38: batch 208/265] [d_loss: 0.017670] [g_loss: 0.034777]\n",
            "[Epoch 38: batch 218/265] [d_loss: 0.013505] [g_loss: 0.032054]\n",
            "[Epoch 38: batch 228/265] [d_loss: 0.019897] [g_loss: 0.030432]\n",
            "[Epoch 38: batch 238/265] [d_loss: 0.012643] [g_loss: 0.028046]\n",
            "[Epoch 38: batch 248/265] [d_loss: 0.495485] [g_loss: 0.024811]\n",
            "[Epoch 38: batch 258/265] [d_loss: 0.035599] [g_loss: 0.031228]\n",
            "[Epoch 39: batch 4/265] [d_loss: 0.026606] [g_loss: 0.035984]\n",
            "[Epoch 39: batch 14/265] [d_loss: 0.392954] [g_loss: 0.027656]\n",
            "[Epoch 39: batch 24/265] [d_loss: 0.000692] [g_loss: 0.029933]\n",
            "[Epoch 39: batch 34/265] [d_loss: 0.469059] [g_loss: 0.024035]\n",
            "[Epoch 39: batch 44/265] [d_loss: 0.005601] [g_loss: 0.035137]\n",
            "[Epoch 39: batch 54/265] [d_loss: 0.007646] [g_loss: 0.024767]\n",
            "[Epoch 39: batch 64/265] [d_loss: 0.006412] [g_loss: 0.032982]\n",
            "[Epoch 39: batch 74/265] [d_loss: 0.000496] [g_loss: 0.037042]\n",
            "[Epoch 39: batch 84/265] [d_loss: 0.072808] [g_loss: 0.028393]\n",
            "[Epoch 39: batch 94/265] [d_loss: 0.019076] [g_loss: 0.028625]\n",
            "[Epoch 39: batch 104/265] [d_loss: 0.067055] [g_loss: 0.028375]\n",
            "[Epoch 39: batch 114/265] [d_loss: 0.454166] [g_loss: 0.027407]\n",
            "[Epoch 39: batch 124/265] [d_loss: 0.028263] [g_loss: 0.031670]\n",
            "[Epoch 39: batch 134/265] [d_loss: 0.011839] [g_loss: 0.032998]\n",
            "[Epoch 39: batch 144/265] [d_loss: 0.001041] [g_loss: 0.028601]\n",
            "[Epoch 39: batch 154/265] [d_loss: 0.006348] [g_loss: 0.033224]\n",
            "[Epoch 39: batch 164/265] [d_loss: 0.014917] [g_loss: 0.026356]\n",
            "[Epoch 39: batch 174/265] [d_loss: 0.122677] [g_loss: 0.032034]\n",
            "[Epoch 39: batch 184/265] [d_loss: 0.019540] [g_loss: 0.037379]\n",
            "[Epoch 39: batch 194/265] [d_loss: 0.141629] [g_loss: 0.035761]\n",
            "[Epoch 39: batch 204/265] [d_loss: 0.065762] [g_loss: 0.042685]\n",
            "[Epoch 39: batch 214/265] [d_loss: 0.015416] [g_loss: 0.028046]\n",
            "[Epoch 39: batch 224/265] [d_loss: 0.492988] [g_loss: 0.031848]\n",
            "[Epoch 39: batch 234/265] [d_loss: 0.421185] [g_loss: 0.027831]\n",
            "[Epoch 39: batch 244/265] [d_loss: 0.356117] [g_loss: 0.027939]\n",
            "[Epoch 39: batch 254/265] [d_loss: 0.087854] [g_loss: 0.030549]\n",
            "[Epoch 39: batch 264/265] [d_loss: 0.011812] [g_loss: 0.039940]\n",
            "\n",
            "Saved trained model in /content/drive/MyDrive/SRDRM/checkpoints/USR_4x/srdrm-gan\n",
            "\n",
            "[Epoch 40: batch 10/265] [d_loss: 0.011524] [g_loss: 0.036110]\n",
            "[Epoch 40: batch 20/265] [d_loss: 0.007832] [g_loss: 0.027678]\n",
            "[Epoch 40: batch 30/265] [d_loss: 0.011580] [g_loss: 0.029194]\n",
            "[Epoch 40: batch 40/265] [d_loss: 0.457605] [g_loss: 0.026643]\n",
            "[Epoch 40: batch 50/265] [d_loss: 0.000648] [g_loss: 0.035739]\n",
            "[Epoch 40: batch 60/265] [d_loss: 0.007408] [g_loss: 0.028223]\n",
            "[Epoch 40: batch 70/265] [d_loss: 0.000836] [g_loss: 0.037196]\n",
            "[Epoch 40: batch 80/265] [d_loss: 0.038473] [g_loss: 0.031039]\n",
            "[Epoch 40: batch 90/265] [d_loss: 0.045068] [g_loss: 0.038633]\n",
            "[Epoch 40: batch 100/265] [d_loss: 0.000855] [g_loss: 0.038887]\n",
            "[Epoch 40: batch 110/265] [d_loss: 0.001012] [g_loss: 0.030222]\n",
            "[Epoch 40: batch 120/265] [d_loss: 0.002812] [g_loss: 0.033222]\n",
            "[Epoch 40: batch 130/265] [d_loss: 0.006048] [g_loss: 0.032180]\n",
            "[Epoch 40: batch 140/265] [d_loss: 0.433451] [g_loss: 0.032277]\n",
            "[Epoch 40: batch 150/265] [d_loss: 0.178892] [g_loss: 0.037172]\n",
            "[Epoch 40: batch 160/265] [d_loss: 0.243699] [g_loss: 0.026612]\n",
            "[Epoch 40: batch 170/265] [d_loss: 0.010829] [g_loss: 0.029837]\n",
            "[Epoch 40: batch 180/265] [d_loss: 0.013851] [g_loss: 0.034116]\n",
            "[Epoch 40: batch 190/265] [d_loss: 0.356736] [g_loss: 0.031932]\n",
            "[Epoch 40: batch 200/265] [d_loss: 0.002399] [g_loss: 0.031121]\n",
            "[Epoch 40: batch 210/265] [d_loss: 0.006854] [g_loss: 0.034608]\n",
            "[Epoch 40: batch 220/265] [d_loss: 0.070937] [g_loss: 0.031164]\n",
            "[Epoch 40: batch 230/265] [d_loss: 0.008284] [g_loss: 0.026422]\n",
            "[Epoch 40: batch 240/265] [d_loss: 0.010537] [g_loss: 0.031676]\n",
            "[Epoch 40: batch 250/265] [d_loss: 0.036668] [g_loss: 0.034974]\n",
            "[Epoch 40: batch 260/265] [d_loss: 0.023834] [g_loss: 0.035703]\n",
            "[Epoch 41: batch 6/265] [d_loss: 0.038722] [g_loss: 0.029913]\n",
            "[Epoch 41: batch 16/265] [d_loss: 0.001235] [g_loss: 0.033643]\n",
            "[Epoch 41: batch 26/265] [d_loss: 0.150336] [g_loss: 0.043246]\n",
            "[Epoch 41: batch 36/265] [d_loss: 0.008835] [g_loss: 0.023474]\n",
            "[Epoch 41: batch 46/265] [d_loss: 0.015545] [g_loss: 0.033261]\n",
            "[Epoch 41: batch 56/265] [d_loss: 0.133560] [g_loss: 0.028223]\n",
            "[Epoch 41: batch 66/265] [d_loss: 0.031469] [g_loss: 0.027575]\n",
            "[Epoch 41: batch 76/265] [d_loss: 0.156746] [g_loss: 0.032264]\n",
            "[Epoch 41: batch 86/265] [d_loss: 0.003976] [g_loss: 0.037266]\n",
            "[Epoch 41: batch 96/265] [d_loss: 0.124871] [g_loss: 0.022074]\n",
            "[Epoch 41: batch 106/265] [d_loss: 0.394656] [g_loss: 0.031444]\n",
            "[Epoch 41: batch 116/265] [d_loss: 0.017904] [g_loss: 0.024410]\n",
            "[Epoch 41: batch 126/265] [d_loss: 0.162630] [g_loss: 0.029709]\n",
            "[Epoch 41: batch 136/265] [d_loss: 0.000787] [g_loss: 0.028573]\n",
            "[Epoch 41: batch 146/265] [d_loss: 0.010706] [g_loss: 0.029403]\n",
            "[Epoch 41: batch 156/265] [d_loss: 0.001567] [g_loss: 0.026484]\n",
            "[Epoch 41: batch 166/265] [d_loss: 0.012204] [g_loss: 0.031106]\n",
            "[Epoch 41: batch 176/265] [d_loss: 0.027191] [g_loss: 0.028435]\n",
            "[Epoch 41: batch 186/265] [d_loss: 0.036447] [g_loss: 0.029748]\n",
            "[Epoch 41: batch 196/265] [d_loss: 0.001002] [g_loss: 0.029870]\n",
            "[Epoch 41: batch 206/265] [d_loss: 0.000869] [g_loss: 0.028224]\n",
            "[Epoch 41: batch 216/265] [d_loss: 0.004136] [g_loss: 0.023804]\n",
            "[Epoch 41: batch 226/265] [d_loss: 0.000956] [g_loss: 0.023313]\n",
            "[Epoch 41: batch 236/265] [d_loss: 0.372174] [g_loss: 0.030203]\n",
            "[Epoch 41: batch 246/265] [d_loss: 0.000641] [g_loss: 0.027792]\n",
            "[Epoch 41: batch 256/265] [d_loss: 0.000662] [g_loss: 0.029484]\n",
            "[Epoch 42: batch 2/265] [d_loss: 0.022831] [g_loss: 0.025747]\n",
            "[Epoch 42: batch 12/265] [d_loss: 0.350611] [g_loss: 0.028523]\n",
            "[Epoch 42: batch 22/265] [d_loss: 0.007998] [g_loss: 0.035577]\n",
            "[Epoch 42: batch 32/265] [d_loss: 0.002542] [g_loss: 0.032984]\n",
            "[Epoch 42: batch 42/265] [d_loss: 0.041848] [g_loss: 0.027203]\n",
            "[Epoch 42: batch 52/265] [d_loss: 0.001955] [g_loss: 0.031460]\n",
            "[Epoch 42: batch 62/265] [d_loss: 0.010546] [g_loss: 0.031580]\n",
            "[Epoch 42: batch 72/265] [d_loss: 0.035583] [g_loss: 0.027743]\n",
            "[Epoch 42: batch 82/265] [d_loss: 0.132869] [g_loss: 0.026308]\n",
            "[Epoch 42: batch 92/265] [d_loss: 0.427873] [g_loss: 0.026897]\n",
            "[Epoch 42: batch 102/265] [d_loss: 0.003187] [g_loss: 0.029064]\n",
            "[Epoch 42: batch 112/265] [d_loss: 0.002512] [g_loss: 0.031402]\n",
            "[Epoch 42: batch 122/265] [d_loss: 0.053562] [g_loss: 0.025860]\n",
            "[Epoch 42: batch 132/265] [d_loss: 0.293889] [g_loss: 0.033474]\n",
            "[Epoch 42: batch 142/265] [d_loss: 0.147126] [g_loss: 0.025477]\n",
            "[Epoch 42: batch 152/265] [d_loss: 0.007879] [g_loss: 0.030839]\n",
            "[Epoch 42: batch 162/265] [d_loss: 0.005147] [g_loss: 0.022928]\n",
            "[Epoch 42: batch 172/265] [d_loss: 0.007916] [g_loss: 0.030788]\n",
            "[Epoch 42: batch 182/265] [d_loss: 0.033065] [g_loss: 0.026044]\n",
            "[Epoch 42: batch 192/265] [d_loss: 0.010217] [g_loss: 0.027900]\n",
            "[Epoch 42: batch 202/265] [d_loss: 0.017750] [g_loss: 0.028446]\n",
            "[Epoch 42: batch 212/265] [d_loss: 0.004631] [g_loss: 0.025984]\n",
            "[Epoch 42: batch 222/265] [d_loss: 0.006819] [g_loss: 0.023441]\n",
            "[Epoch 42: batch 232/265] [d_loss: 0.082411] [g_loss: 0.030266]\n",
            "[Epoch 42: batch 242/265] [d_loss: 0.004072] [g_loss: 0.034592]\n",
            "[Epoch 42: batch 252/265] [d_loss: 0.001482] [g_loss: 0.036465]\n",
            "[Epoch 42: batch 262/265] [d_loss: 0.048643] [g_loss: 0.031851]\n",
            "[Epoch 43: batch 8/265] [d_loss: 0.566842] [g_loss: 0.036059]\n",
            "[Epoch 43: batch 18/265] [d_loss: 0.004916] [g_loss: 0.026646]\n",
            "[Epoch 43: batch 28/265] [d_loss: 0.155424] [g_loss: 0.023212]\n",
            "[Epoch 43: batch 38/265] [d_loss: 0.004858] [g_loss: 0.031703]\n",
            "[Epoch 43: batch 48/265] [d_loss: 0.034598] [g_loss: 0.030373]\n",
            "[Epoch 43: batch 58/265] [d_loss: 0.037711] [g_loss: 0.021929]\n",
            "[Epoch 43: batch 68/265] [d_loss: 0.008561] [g_loss: 0.023884]\n",
            "[Epoch 43: batch 78/265] [d_loss: 0.016444] [g_loss: 0.025538]\n",
            "[Epoch 43: batch 88/265] [d_loss: 0.015694] [g_loss: 0.032592]\n",
            "[Epoch 43: batch 98/265] [d_loss: 0.073596] [g_loss: 0.032001]\n",
            "[Epoch 43: batch 108/265] [d_loss: 0.005503] [g_loss: 0.027353]\n",
            "[Epoch 43: batch 118/265] [d_loss: 0.000813] [g_loss: 0.026541]\n",
            "[Epoch 43: batch 128/265] [d_loss: 0.070227] [g_loss: 0.028140]\n",
            "[Epoch 43: batch 138/265] [d_loss: 0.011789] [g_loss: 0.025183]\n",
            "[Epoch 43: batch 148/265] [d_loss: 0.008877] [g_loss: 0.033425]\n",
            "[Epoch 43: batch 158/265] [d_loss: 0.013137] [g_loss: 0.029087]\n",
            "[Epoch 43: batch 168/265] [d_loss: 0.477820] [g_loss: 0.028675]\n",
            "[Epoch 43: batch 178/265] [d_loss: 0.248572] [g_loss: 0.030708]\n",
            "[Epoch 43: batch 188/265] [d_loss: 0.007598] [g_loss: 0.030059]\n",
            "[Epoch 43: batch 198/265] [d_loss: 0.049446] [g_loss: 0.030338]\n",
            "[Epoch 43: batch 208/265] [d_loss: 0.468701] [g_loss: 0.034545]\n",
            "[Epoch 43: batch 218/265] [d_loss: 0.004941] [g_loss: 0.031118]\n",
            "[Epoch 43: batch 228/265] [d_loss: 0.000624] [g_loss: 0.028375]\n",
            "[Epoch 43: batch 238/265] [d_loss: 0.002476] [g_loss: 0.026667]\n",
            "[Epoch 43: batch 248/265] [d_loss: 0.329680] [g_loss: 0.021233]\n",
            "[Epoch 43: batch 258/265] [d_loss: 0.001712] [g_loss: 0.029189]\n",
            "\n",
            "Saved trained model in /content/drive/MyDrive/SRDRM/checkpoints/USR_4x/srdrm-gan\n",
            "\n",
            "[Epoch 44: batch 4/265] [d_loss: 0.000991] [g_loss: 0.035248]\n",
            "[Epoch 44: batch 14/265] [d_loss: 0.402689] [g_loss: 0.023022]\n",
            "[Epoch 44: batch 24/265] [d_loss: 0.016711] [g_loss: 0.029022]\n",
            "[Epoch 44: batch 34/265] [d_loss: 0.420374] [g_loss: 0.027498]\n",
            "[Epoch 44: batch 44/265] [d_loss: 0.018583] [g_loss: 0.033152]\n",
            "[Epoch 44: batch 54/265] [d_loss: 0.043520] [g_loss: 0.024290]\n",
            "[Epoch 44: batch 64/265] [d_loss: 0.001021] [g_loss: 0.033501]\n",
            "[Epoch 44: batch 74/265] [d_loss: 0.124858] [g_loss: 0.035173]\n",
            "[Epoch 44: batch 84/265] [d_loss: 0.004568] [g_loss: 0.028086]\n",
            "[Epoch 44: batch 94/265] [d_loss: 0.414417] [g_loss: 0.031735]\n",
            "[Epoch 44: batch 104/265] [d_loss: 0.277804] [g_loss: 0.026220]\n",
            "[Epoch 44: batch 114/265] [d_loss: 0.382027] [g_loss: 0.026124]\n",
            "[Epoch 44: batch 124/265] [d_loss: 0.294831] [g_loss: 0.031108]\n",
            "[Epoch 44: batch 134/265] [d_loss: 0.036742] [g_loss: 0.029763]\n",
            "[Epoch 44: batch 144/265] [d_loss: 0.309734] [g_loss: 0.025649]\n",
            "[Epoch 44: batch 154/265] [d_loss: 0.007126] [g_loss: 0.032837]\n",
            "[Epoch 44: batch 164/265] [d_loss: 0.008841] [g_loss: 0.026972]\n",
            "[Epoch 44: batch 174/265] [d_loss: 0.292377] [g_loss: 0.031068]\n",
            "[Epoch 44: batch 184/265] [d_loss: 0.008932] [g_loss: 0.034558]\n",
            "[Epoch 44: batch 194/265] [d_loss: 0.047186] [g_loss: 0.032017]\n",
            "[Epoch 44: batch 204/265] [d_loss: 0.000594] [g_loss: 0.040520]\n",
            "[Epoch 44: batch 214/265] [d_loss: 0.020687] [g_loss: 0.026611]\n",
            "[Epoch 44: batch 224/265] [d_loss: 0.003210] [g_loss: 0.031814]\n",
            "[Epoch 44: batch 234/265] [d_loss: 0.029687] [g_loss: 0.029099]\n",
            "[Epoch 44: batch 244/265] [d_loss: 0.420416] [g_loss: 0.027643]\n",
            "[Epoch 44: batch 254/265] [d_loss: 0.009967] [g_loss: 0.029977]\n",
            "[Epoch 44: batch 264/265] [d_loss: 0.011665] [g_loss: 0.040918]\n",
            "[Epoch 45: batch 10/265] [d_loss: 0.021565] [g_loss: 0.036965]\n",
            "[Epoch 45: batch 20/265] [d_loss: 0.300917] [g_loss: 0.030155]\n",
            "[Epoch 45: batch 30/265] [d_loss: 0.002670] [g_loss: 0.031175]\n",
            "[Epoch 45: batch 40/265] [d_loss: 0.467109] [g_loss: 0.030263]\n",
            "[Epoch 45: batch 50/265] [d_loss: 0.004171] [g_loss: 0.033776]\n",
            "[Epoch 45: batch 60/265] [d_loss: 0.004556] [g_loss: 0.029024]\n",
            "[Epoch 45: batch 70/265] [d_loss: 0.005709] [g_loss: 0.036568]\n",
            "[Epoch 45: batch 80/265] [d_loss: 0.002982] [g_loss: 0.030098]\n",
            "[Epoch 45: batch 90/265] [d_loss: 0.106958] [g_loss: 0.025466]\n",
            "[Epoch 45: batch 100/265] [d_loss: 0.029793] [g_loss: 0.032538]\n",
            "[Epoch 45: batch 110/265] [d_loss: 0.008941] [g_loss: 0.031660]\n",
            "[Epoch 45: batch 120/265] [d_loss: 0.429515] [g_loss: 0.030570]\n",
            "[Epoch 45: batch 130/265] [d_loss: 0.035815] [g_loss: 0.031256]\n",
            "[Epoch 45: batch 140/265] [d_loss: 0.417488] [g_loss: 0.029826]\n",
            "[Epoch 45: batch 150/265] [d_loss: 0.011773] [g_loss: 0.036876]\n",
            "[Epoch 45: batch 160/265] [d_loss: 0.247515] [g_loss: 0.026669]\n",
            "[Epoch 45: batch 170/265] [d_loss: 0.012940] [g_loss: 0.028506]\n",
            "[Epoch 45: batch 180/265] [d_loss: 0.002551] [g_loss: 0.030767]\n",
            "[Epoch 45: batch 190/265] [d_loss: 0.143294] [g_loss: 0.028636]\n",
            "[Epoch 45: batch 200/265] [d_loss: 0.036776] [g_loss: 0.032401]\n",
            "[Epoch 45: batch 210/265] [d_loss: 0.008068] [g_loss: 0.032503]\n",
            "[Epoch 45: batch 220/265] [d_loss: 0.006084] [g_loss: 0.030141]\n",
            "[Epoch 45: batch 230/265] [d_loss: 0.026569] [g_loss: 0.026371]\n",
            "[Epoch 45: batch 240/265] [d_loss: 0.013737] [g_loss: 0.031212]\n",
            "[Epoch 45: batch 250/265] [d_loss: 0.068690] [g_loss: 0.033287]\n",
            "[Epoch 45: batch 260/265] [d_loss: 0.000871] [g_loss: 0.032212]\n",
            "[Epoch 46: batch 6/265] [d_loss: 0.073518] [g_loss: 0.028381]\n",
            "[Epoch 46: batch 16/265] [d_loss: 0.301159] [g_loss: 0.033132]\n",
            "[Epoch 46: batch 26/265] [d_loss: 0.085026] [g_loss: 0.040242]\n",
            "[Epoch 46: batch 36/265] [d_loss: 0.047047] [g_loss: 0.023867]\n",
            "[Epoch 46: batch 46/265] [d_loss: 0.025080] [g_loss: 0.032716]\n",
            "[Epoch 46: batch 56/265] [d_loss: 0.021376] [g_loss: 0.023997]\n",
            "[Epoch 46: batch 66/265] [d_loss: 0.022819] [g_loss: 0.026645]\n",
            "[Epoch 46: batch 76/265] [d_loss: 0.009970] [g_loss: 0.030582]\n",
            "[Epoch 46: batch 86/265] [d_loss: 0.004143] [g_loss: 0.038830]\n",
            "[Epoch 46: batch 96/265] [d_loss: 0.014976] [g_loss: 0.023165]\n",
            "[Epoch 46: batch 106/265] [d_loss: 0.128698] [g_loss: 0.030150]\n",
            "[Epoch 46: batch 116/265] [d_loss: 0.559220] [g_loss: 0.025598]\n",
            "[Epoch 46: batch 126/265] [d_loss: 0.317553] [g_loss: 0.028231]\n",
            "[Epoch 46: batch 136/265] [d_loss: 0.006662] [g_loss: 0.027008]\n",
            "[Epoch 46: batch 146/265] [d_loss: 0.031179] [g_loss: 0.027597]\n",
            "[Epoch 46: batch 156/265] [d_loss: 0.002954] [g_loss: 0.028151]\n",
            "[Epoch 46: batch 166/265] [d_loss: 0.006545] [g_loss: 0.035602]\n",
            "[Epoch 46: batch 176/265] [d_loss: 0.018461] [g_loss: 0.027216]\n",
            "[Epoch 46: batch 186/265] [d_loss: 0.403229] [g_loss: 0.026874]\n",
            "[Epoch 46: batch 196/265] [d_loss: 0.196502] [g_loss: 0.029507]\n",
            "[Epoch 46: batch 206/265] [d_loss: 0.001174] [g_loss: 0.028031]\n",
            "[Epoch 46: batch 216/265] [d_loss: 0.024254] [g_loss: 0.024956]\n",
            "[Epoch 46: batch 226/265] [d_loss: 0.010228] [g_loss: 0.026759]\n",
            "[Epoch 46: batch 236/265] [d_loss: 0.010121] [g_loss: 0.032882]\n",
            "[Epoch 46: batch 246/265] [d_loss: 0.006564] [g_loss: 0.028233]\n",
            "[Epoch 46: batch 256/265] [d_loss: 0.003906] [g_loss: 0.028149]\n",
            "[Epoch 47: batch 2/265] [d_loss: 0.359743] [g_loss: 0.024093]\n",
            "[Epoch 47: batch 12/265] [d_loss: 0.407259] [g_loss: 0.025960]\n",
            "[Epoch 47: batch 22/265] [d_loss: 0.007386] [g_loss: 0.033990]\n",
            "[Epoch 47: batch 32/265] [d_loss: 0.001117] [g_loss: 0.032208]\n",
            "[Epoch 47: batch 42/265] [d_loss: 0.018950] [g_loss: 0.026254]\n",
            "[Epoch 47: batch 52/265] [d_loss: 0.047404] [g_loss: 0.029355]\n",
            "[Epoch 47: batch 62/265] [d_loss: 0.007051] [g_loss: 0.032603]\n",
            "[Epoch 47: batch 72/265] [d_loss: 0.047964] [g_loss: 0.026295]\n",
            "[Epoch 47: batch 82/265] [d_loss: 0.071869] [g_loss: 0.028242]\n",
            "[Epoch 47: batch 92/265] [d_loss: 0.050342] [g_loss: 0.027220]\n",
            "[Epoch 47: batch 102/265] [d_loss: 0.015841] [g_loss: 0.027977]\n",
            "[Epoch 47: batch 112/265] [d_loss: 0.021022] [g_loss: 0.031744]\n",
            "[Epoch 47: batch 122/265] [d_loss: 0.018717] [g_loss: 0.026688]\n",
            "[Epoch 47: batch 132/265] [d_loss: 0.106778] [g_loss: 0.032780]\n",
            "[Epoch 47: batch 142/265] [d_loss: 0.000566] [g_loss: 0.024640]\n",
            "[Epoch 47: batch 152/265] [d_loss: 0.002434] [g_loss: 0.029432]\n",
            "[Epoch 47: batch 162/265] [d_loss: 0.002202] [g_loss: 0.023709]\n",
            "[Epoch 47: batch 172/265] [d_loss: 0.291410] [g_loss: 0.029884]\n",
            "[Epoch 47: batch 182/265] [d_loss: 0.001516] [g_loss: 0.024686]\n",
            "[Epoch 47: batch 192/265] [d_loss: 0.000872] [g_loss: 0.026524]\n",
            "[Epoch 47: batch 202/265] [d_loss: 0.020935] [g_loss: 0.027386]\n",
            "[Epoch 47: batch 212/265] [d_loss: 0.000416] [g_loss: 0.057778]\n",
            "[Epoch 47: batch 222/265] [d_loss: 0.004880] [g_loss: 0.033872]\n",
            "[Epoch 47: batch 232/265] [d_loss: 0.384538] [g_loss: 0.033477]\n",
            "[Epoch 47: batch 242/265] [d_loss: 0.003152] [g_loss: 0.039260]\n",
            "[Epoch 47: batch 252/265] [d_loss: 0.005457] [g_loss: 0.039439]\n",
            "[Epoch 47: batch 262/265] [d_loss: 0.017349] [g_loss: 0.031077]\n",
            "\n",
            "Saved trained model in /content/drive/MyDrive/SRDRM/checkpoints/USR_4x/srdrm-gan\n",
            "\n",
            "[Epoch 48: batch 8/265] [d_loss: 0.043182] [g_loss: 0.034195]\n",
            "[Epoch 48: batch 18/265] [d_loss: 0.061521] [g_loss: 0.029210]\n",
            "[Epoch 48: batch 28/265] [d_loss: 0.000404] [g_loss: 0.028678]\n",
            "[Epoch 48: batch 38/265] [d_loss: 0.003151] [g_loss: 0.035275]\n",
            "[Epoch 48: batch 48/265] [d_loss: 0.007480] [g_loss: 0.036579]\n",
            "[Epoch 48: batch 58/265] [d_loss: 0.002090] [g_loss: 0.025484]\n",
            "[Epoch 48: batch 68/265] [d_loss: 0.002511] [g_loss: 0.025512]\n",
            "[Epoch 48: batch 78/265] [d_loss: 0.000242] [g_loss: 0.027272]\n",
            "[Epoch 48: batch 88/265] [d_loss: 0.001169] [g_loss: 0.040596]\n",
            "[Epoch 48: batch 98/265] [d_loss: 0.000316] [g_loss: 0.033066]\n",
            "[Epoch 48: batch 108/265] [d_loss: 0.000334] [g_loss: 0.031954]\n",
            "[Epoch 48: batch 118/265] [d_loss: 0.001238] [g_loss: 0.026556]\n",
            "[Epoch 48: batch 128/265] [d_loss: 0.002208] [g_loss: 0.032682]\n",
            "[Epoch 48: batch 138/265] [d_loss: 0.000471] [g_loss: 0.030451]\n",
            "[Epoch 48: batch 148/265] [d_loss: 0.000396] [g_loss: 0.035048]\n",
            "[Epoch 48: batch 158/265] [d_loss: 0.001718] [g_loss: 0.034521]\n",
            "[Epoch 48: batch 168/265] [d_loss: 0.437387] [g_loss: 0.030340]\n",
            "[Epoch 48: batch 178/265] [d_loss: 0.022690] [g_loss: 0.034424]\n",
            "[Epoch 48: batch 188/265] [d_loss: 0.001422] [g_loss: 0.031577]\n",
            "[Epoch 48: batch 198/265] [d_loss: 0.009957] [g_loss: 0.032448]\n",
            "[Epoch 48: batch 208/265] [d_loss: 0.010284] [g_loss: 0.033433]\n",
            "[Epoch 48: batch 218/265] [d_loss: 0.011376] [g_loss: 0.037675]\n",
            "[Epoch 48: batch 228/265] [d_loss: 0.000651] [g_loss: 0.032760]\n",
            "[Epoch 48: batch 238/265] [d_loss: 0.001655] [g_loss: 0.027721]\n",
            "[Epoch 48: batch 248/265] [d_loss: 0.370087] [g_loss: 0.024542]\n",
            "[Epoch 48: batch 258/265] [d_loss: 0.000111] [g_loss: 0.032489]\n",
            "[Epoch 49: batch 4/265] [d_loss: 0.000284] [g_loss: 0.040582]\n",
            "[Epoch 49: batch 14/265] [d_loss: 0.461083] [g_loss: 0.025458]\n",
            "[Epoch 49: batch 24/265] [d_loss: 0.000520] [g_loss: 0.035037]\n",
            "[Epoch 49: batch 34/265] [d_loss: 0.031451] [g_loss: 0.025826]\n",
            "[Epoch 49: batch 44/265] [d_loss: 0.002421] [g_loss: 0.035592]\n",
            "[Epoch 49: batch 54/265] [d_loss: 0.032180] [g_loss: 0.026314]\n",
            "[Epoch 49: batch 64/265] [d_loss: 0.003181] [g_loss: 0.035999]\n",
            "[Epoch 49: batch 74/265] [d_loss: 0.005504] [g_loss: 0.035182]\n",
            "[Epoch 49: batch 84/265] [d_loss: 0.001292] [g_loss: 0.032705]\n",
            "[Epoch 49: batch 94/265] [d_loss: 0.099609] [g_loss: 0.027533]\n",
            "[Epoch 49: batch 104/265] [d_loss: 0.287153] [g_loss: 0.025525]\n",
            "[Epoch 49: batch 114/265] [d_loss: 0.125146] [g_loss: 0.027202]\n",
            "[Epoch 49: batch 124/265] [d_loss: 0.039182] [g_loss: 0.031492]\n",
            "[Epoch 49: batch 134/265] [d_loss: 0.001486] [g_loss: 0.032626]\n",
            "[Epoch 49: batch 144/265] [d_loss: 0.264141] [g_loss: 0.027679]\n",
            "[Epoch 49: batch 154/265] [d_loss: 0.005350] [g_loss: 0.028786]\n",
            "[Epoch 49: batch 164/265] [d_loss: 0.045094] [g_loss: 0.028377]\n",
            "[Epoch 49: batch 174/265] [d_loss: 0.420309] [g_loss: 0.033479]\n",
            "[Epoch 49: batch 184/265] [d_loss: 0.016373] [g_loss: 0.036798]\n",
            "[Epoch 49: batch 194/265] [d_loss: 0.012188] [g_loss: 0.033278]\n",
            "[Epoch 49: batch 204/265] [d_loss: 0.000650] [g_loss: 0.041065]\n",
            "[Epoch 49: batch 214/265] [d_loss: 0.001092] [g_loss: 0.028296]\n",
            "[Epoch 49: batch 224/265] [d_loss: 0.000608] [g_loss: 0.030801]\n",
            "[Epoch 49: batch 234/265] [d_loss: 0.374625] [g_loss: 0.031058]\n",
            "[Epoch 49: batch 244/265] [d_loss: 0.004449] [g_loss: 0.030066]\n",
            "[Epoch 49: batch 254/265] [d_loss: 0.000467] [g_loss: 0.032009]\n",
            "[Epoch 49: batch 264/265] [d_loss: 0.027490] [g_loss: 0.042012]\n",
            "[Epoch 50: batch 10/265] [d_loss: 0.034213] [g_loss: 0.035299]\n",
            "[Epoch 50: batch 20/265] [d_loss: 0.466096] [g_loss: 0.026867]\n",
            "[Epoch 50: batch 30/265] [d_loss: 0.003520] [g_loss: 0.033007]\n",
            "[Epoch 50: batch 40/265] [d_loss: 0.466028] [g_loss: 0.030701]\n",
            "[Epoch 50: batch 50/265] [d_loss: 0.002320] [g_loss: 0.034509]\n",
            "[Epoch 50: batch 60/265] [d_loss: 0.001831] [g_loss: 0.030776]\n",
            "[Epoch 50: batch 70/265] [d_loss: 0.016414] [g_loss: 0.037317]\n",
            "[Epoch 50: batch 80/265] [d_loss: 0.000690] [g_loss: 0.032450]\n",
            "[Epoch 50: batch 90/265] [d_loss: 0.019442] [g_loss: 0.028019]\n",
            "[Epoch 50: batch 100/265] [d_loss: 0.000581] [g_loss: 0.033352]\n",
            "[Epoch 50: batch 110/265] [d_loss: 0.001672] [g_loss: 0.027948]\n",
            "[Epoch 50: batch 120/265] [d_loss: 0.177948] [g_loss: 0.033056]\n",
            "[Epoch 50: batch 130/265] [d_loss: 0.116919] [g_loss: 0.033653]\n",
            "[Epoch 50: batch 140/265] [d_loss: 0.440828] [g_loss: 0.029306]\n",
            "[Epoch 50: batch 150/265] [d_loss: 0.020708] [g_loss: 0.039562]\n",
            "[Epoch 50: batch 160/265] [d_loss: 0.241526] [g_loss: 0.024565]\n",
            "[Epoch 50: batch 170/265] [d_loss: 0.020346] [g_loss: 0.030472]\n",
            "[Epoch 50: batch 180/265] [d_loss: 0.018208] [g_loss: 0.030678]\n",
            "[Epoch 50: batch 190/265] [d_loss: 0.008614] [g_loss: 0.026745]\n",
            "[Epoch 50: batch 200/265] [d_loss: 0.036879] [g_loss: 0.035005]\n",
            "[Epoch 50: batch 210/265] [d_loss: 0.005819] [g_loss: 0.035277]\n",
            "[Epoch 50: batch 220/265] [d_loss: 0.143731] [g_loss: 0.029735]\n",
            "[Epoch 50: batch 230/265] [d_loss: 0.067179] [g_loss: 0.026413]\n",
            "[Epoch 50: batch 240/265] [d_loss: 0.042641] [g_loss: 0.029302]\n",
            "[Epoch 50: batch 250/265] [d_loss: 0.481540] [g_loss: 0.036626]\n",
            "[Epoch 50: batch 260/265] [d_loss: 0.000789] [g_loss: 0.037236]\n",
            "[Epoch 51: batch 6/265] [d_loss: 0.004399] [g_loss: 0.030995]\n",
            "[Epoch 51: batch 16/265] [d_loss: 0.007792] [g_loss: 0.032876]\n",
            "[Epoch 51: batch 26/265] [d_loss: 0.003162] [g_loss: 0.040416]\n",
            "[Epoch 51: batch 36/265] [d_loss: 0.306872] [g_loss: 0.025544]\n",
            "[Epoch 51: batch 46/265] [d_loss: 0.056982] [g_loss: 0.032149]\n",
            "[Epoch 51: batch 56/265] [d_loss: 0.007980] [g_loss: 0.025393]\n",
            "[Epoch 51: batch 66/265] [d_loss: 0.021865] [g_loss: 0.028041]\n",
            "[Epoch 51: batch 76/265] [d_loss: 0.133515] [g_loss: 0.032130]\n",
            "[Epoch 51: batch 86/265] [d_loss: 0.002037] [g_loss: 0.036939]\n",
            "[Epoch 51: batch 96/265] [d_loss: 0.004199] [g_loss: 0.025992]\n",
            "[Epoch 51: batch 106/265] [d_loss: 0.007825] [g_loss: 0.029534]\n",
            "[Epoch 51: batch 116/265] [d_loss: 0.047312] [g_loss: 0.024698]\n",
            "[Epoch 51: batch 126/265] [d_loss: 0.065538] [g_loss: 0.029238]\n",
            "[Epoch 51: batch 136/265] [d_loss: 0.004043] [g_loss: 0.028137]\n",
            "[Epoch 51: batch 146/265] [d_loss: 0.003342] [g_loss: 0.029262]\n",
            "[Epoch 51: batch 156/265] [d_loss: 0.002050] [g_loss: 0.026811]\n",
            "[Epoch 51: batch 166/265] [d_loss: 0.008900] [g_loss: 0.031199]\n",
            "[Epoch 51: batch 176/265] [d_loss: 0.023031] [g_loss: 0.027716]\n",
            "[Epoch 51: batch 186/265] [d_loss: 0.002563] [g_loss: 0.029745]\n",
            "[Epoch 51: batch 196/265] [d_loss: 0.038523] [g_loss: 0.027808]\n",
            "[Epoch 51: batch 206/265] [d_loss: 0.037105] [g_loss: 0.026473]\n",
            "[Epoch 51: batch 216/265] [d_loss: 0.051661] [g_loss: 0.025416]\n",
            "[Epoch 51: batch 226/265] [d_loss: 0.011719] [g_loss: 0.025213]\n",
            "[Epoch 51: batch 236/265] [d_loss: 0.022028] [g_loss: 0.030991]\n",
            "[Epoch 51: batch 246/265] [d_loss: 0.277160] [g_loss: 0.026249]\n",
            "[Epoch 51: batch 256/265] [d_loss: 0.004323] [g_loss: 0.029312]\n",
            "\n",
            "Saved trained model in /content/drive/MyDrive/SRDRM/checkpoints/USR_4x/srdrm-gan\n",
            "\n",
            "[Epoch 52: batch 2/265] [d_loss: 0.137604] [g_loss: 0.026265]\n",
            "[Epoch 52: batch 12/265] [d_loss: 0.012047] [g_loss: 0.026883]\n",
            "[Epoch 52: batch 22/265] [d_loss: 0.003827] [g_loss: 0.036984]\n",
            "[Epoch 52: batch 32/265] [d_loss: 0.003946] [g_loss: 0.033327]\n",
            "[Epoch 52: batch 42/265] [d_loss: 0.010453] [g_loss: 0.028520]\n",
            "[Epoch 52: batch 52/265] [d_loss: 0.013021] [g_loss: 0.029850]\n",
            "[Epoch 52: batch 62/265] [d_loss: 0.013542] [g_loss: 0.032944]\n",
            "[Epoch 52: batch 72/265] [d_loss: 0.161451] [g_loss: 0.024568]\n",
            "[Epoch 52: batch 82/265] [d_loss: 0.336672] [g_loss: 0.034092]\n",
            "[Epoch 52: batch 92/265] [d_loss: 0.160363] [g_loss: 0.030272]\n",
            "[Epoch 52: batch 102/265] [d_loss: 0.023125] [g_loss: 0.028432]\n",
            "[Epoch 52: batch 112/265] [d_loss: 0.007458] [g_loss: 0.031767]\n",
            "[Epoch 52: batch 122/265] [d_loss: 0.004757] [g_loss: 0.027482]\n",
            "[Epoch 52: batch 132/265] [d_loss: 0.326860] [g_loss: 0.034504]\n",
            "[Epoch 52: batch 142/265] [d_loss: 0.277311] [g_loss: 0.025540]\n",
            "[Epoch 52: batch 152/265] [d_loss: 0.133638] [g_loss: 0.034323]\n",
            "[Epoch 52: batch 162/265] [d_loss: 0.150780] [g_loss: 0.025720]\n",
            "[Epoch 52: batch 172/265] [d_loss: 0.327392] [g_loss: 0.028263]\n",
            "[Epoch 52: batch 182/265] [d_loss: 0.007638] [g_loss: 0.024531]\n",
            "[Epoch 52: batch 192/265] [d_loss: 0.012026] [g_loss: 0.027601]\n",
            "[Epoch 52: batch 202/265] [d_loss: 0.185868] [g_loss: 0.026083]\n",
            "[Epoch 52: batch 212/265] [d_loss: 0.006219] [g_loss: 0.025761]\n",
            "[Epoch 52: batch 222/265] [d_loss: 0.009023] [g_loss: 0.023633]\n",
            "[Epoch 52: batch 232/265] [d_loss: 0.102240] [g_loss: 0.028385]\n",
            "[Epoch 52: batch 242/265] [d_loss: 0.014046] [g_loss: 0.037616]\n",
            "[Epoch 52: batch 252/265] [d_loss: 0.007738] [g_loss: 0.036816]\n",
            "[Epoch 52: batch 262/265] [d_loss: 0.033025] [g_loss: 0.028834]\n",
            "[Epoch 53: batch 8/265] [d_loss: 0.421923] [g_loss: 0.032407]\n",
            "[Epoch 53: batch 18/265] [d_loss: 0.003044] [g_loss: 0.025743]\n",
            "[Epoch 53: batch 28/265] [d_loss: 0.010890] [g_loss: 0.025804]\n",
            "[Epoch 53: batch 38/265] [d_loss: 0.002971] [g_loss: 0.031149]\n",
            "[Epoch 53: batch 48/265] [d_loss: 0.000333] [g_loss: 0.031283]\n",
            "[Epoch 53: batch 58/265] [d_loss: 0.172095] [g_loss: 0.022325]\n",
            "[Epoch 53: batch 68/265] [d_loss: 0.093631] [g_loss: 0.024659]\n",
            "[Epoch 53: batch 78/265] [d_loss: 0.040138] [g_loss: 0.023927]\n",
            "[Epoch 53: batch 88/265] [d_loss: 0.261791] [g_loss: 0.033997]\n",
            "[Epoch 53: batch 98/265] [d_loss: 0.079156] [g_loss: 0.033029]\n",
            "[Epoch 53: batch 108/265] [d_loss: 0.024977] [g_loss: 0.027571]\n",
            "[Epoch 53: batch 118/265] [d_loss: 0.000935] [g_loss: 0.026154]\n",
            "[Epoch 53: batch 128/265] [d_loss: 0.021010] [g_loss: 0.025880]\n",
            "[Epoch 53: batch 138/265] [d_loss: 0.016721] [g_loss: 0.026483]\n",
            "[Epoch 53: batch 148/265] [d_loss: 0.019966] [g_loss: 0.030230]\n",
            "[Epoch 53: batch 158/265] [d_loss: 0.002256] [g_loss: 0.031772]\n",
            "[Epoch 53: batch 168/265] [d_loss: 0.449565] [g_loss: 0.023595]\n",
            "[Epoch 53: batch 178/265] [d_loss: 0.039586] [g_loss: 0.032827]\n",
            "[Epoch 53: batch 188/265] [d_loss: 0.003942] [g_loss: 0.027475]\n",
            "[Epoch 53: batch 198/265] [d_loss: 0.304733] [g_loss: 0.028389]\n",
            "[Epoch 53: batch 208/265] [d_loss: 0.195438] [g_loss: 0.035776]\n",
            "[Epoch 53: batch 218/265] [d_loss: 0.091701] [g_loss: 0.030227]\n",
            "[Epoch 53: batch 228/265] [d_loss: 0.000566] [g_loss: 0.029455]\n",
            "[Epoch 53: batch 238/265] [d_loss: 0.013067] [g_loss: 0.024133]\n",
            "[Epoch 53: batch 248/265] [d_loss: 0.223371] [g_loss: 0.022168]\n",
            "[Epoch 53: batch 258/265] [d_loss: 0.003203] [g_loss: 0.028680]\n",
            "[Epoch 54: batch 4/265] [d_loss: 0.001486] [g_loss: 0.036015]\n",
            "[Epoch 54: batch 14/265] [d_loss: 0.483491] [g_loss: 0.028451]\n",
            "[Epoch 54: batch 24/265] [d_loss: 0.006882] [g_loss: 0.027838]\n",
            "[Epoch 54: batch 34/265] [d_loss: 0.450982] [g_loss: 0.024098]\n",
            "[Epoch 54: batch 44/265] [d_loss: 0.005596] [g_loss: 0.033236]\n",
            "[Epoch 54: batch 54/265] [d_loss: 0.346145] [g_loss: 0.025974]\n",
            "[Epoch 54: batch 64/265] [d_loss: 0.001381] [g_loss: 0.035191]\n",
            "[Epoch 54: batch 74/265] [d_loss: 0.090657] [g_loss: 0.035861]\n",
            "[Epoch 54: batch 84/265] [d_loss: 0.067923] [g_loss: 0.027799]\n",
            "[Epoch 54: batch 94/265] [d_loss: 0.004942] [g_loss: 0.028046]\n",
            "[Epoch 54: batch 104/265] [d_loss: 0.168147] [g_loss: 0.027355]\n",
            "[Epoch 54: batch 114/265] [d_loss: 0.000201] [g_loss: 0.026918]\n",
            "[Epoch 54: batch 124/265] [d_loss: 0.015720] [g_loss: 0.032307]\n",
            "[Epoch 54: batch 134/265] [d_loss: 0.006390] [g_loss: 0.031276]\n",
            "[Epoch 54: batch 144/265] [d_loss: 0.013238] [g_loss: 0.026726]\n",
            "[Epoch 54: batch 154/265] [d_loss: 0.002766] [g_loss: 0.029990]\n",
            "[Epoch 54: batch 164/265] [d_loss: 0.012686] [g_loss: 0.029227]\n",
            "[Epoch 54: batch 174/265] [d_loss: 0.023386] [g_loss: 0.031476]\n",
            "[Epoch 54: batch 184/265] [d_loss: 0.034548] [g_loss: 0.038575]\n",
            "[Epoch 54: batch 194/265] [d_loss: 0.002964] [g_loss: 0.032587]\n",
            "[Epoch 54: batch 204/265] [d_loss: 0.000268] [g_loss: 0.040936]\n",
            "[Epoch 54: batch 214/265] [d_loss: 0.002770] [g_loss: 0.026398]\n",
            "[Epoch 54: batch 224/265] [d_loss: 0.018891] [g_loss: 0.030130]\n",
            "[Epoch 54: batch 234/265] [d_loss: 0.092661] [g_loss: 0.025862]\n",
            "[Epoch 54: batch 244/265] [d_loss: 0.015303] [g_loss: 0.028031]\n",
            "[Epoch 54: batch 254/265] [d_loss: 0.039419] [g_loss: 0.032030]\n",
            "[Epoch 54: batch 264/265] [d_loss: 0.013918] [g_loss: 0.041510]\n",
            "[Epoch 55: batch 10/265] [d_loss: 0.003219] [g_loss: 0.037914]\n",
            "[Epoch 55: batch 20/265] [d_loss: 0.053324] [g_loss: 0.026849]\n",
            "[Epoch 55: batch 30/265] [d_loss: 0.002287] [g_loss: 0.031017]\n",
            "[Epoch 55: batch 40/265] [d_loss: 0.523984] [g_loss: 0.025970]\n",
            "[Epoch 55: batch 50/265] [d_loss: 0.003823] [g_loss: 0.035287]\n",
            "[Epoch 55: batch 60/265] [d_loss: 0.344076] [g_loss: 0.031267]\n",
            "[Epoch 55: batch 70/265] [d_loss: 0.002194] [g_loss: 0.036772]\n",
            "[Epoch 55: batch 80/265] [d_loss: 0.145150] [g_loss: 0.029346]\n",
            "[Epoch 55: batch 90/265] [d_loss: 0.016023] [g_loss: 0.028477]\n",
            "[Epoch 55: batch 100/265] [d_loss: 0.004061] [g_loss: 0.031831]\n",
            "[Epoch 55: batch 110/265] [d_loss: 0.006665] [g_loss: 0.031301]\n",
            "[Epoch 55: batch 120/265] [d_loss: 0.003783] [g_loss: 0.028975]\n",
            "[Epoch 55: batch 130/265] [d_loss: 0.242376] [g_loss: 0.034253]\n",
            "[Epoch 55: batch 140/265] [d_loss: 0.347136] [g_loss: 0.031026]\n",
            "[Epoch 55: batch 150/265] [d_loss: 0.064667] [g_loss: 0.036561]\n",
            "[Epoch 55: batch 160/265] [d_loss: 0.406475] [g_loss: 0.027751]\n",
            "[Epoch 55: batch 170/265] [d_loss: 0.000626] [g_loss: 0.028400]\n",
            "[Epoch 55: batch 180/265] [d_loss: 0.002833] [g_loss: 0.030899]\n",
            "[Epoch 55: batch 190/265] [d_loss: 0.021909] [g_loss: 0.028355]\n",
            "[Epoch 55: batch 200/265] [d_loss: 0.003711] [g_loss: 0.033136]\n",
            "[Epoch 55: batch 210/265] [d_loss: 0.002168] [g_loss: 0.033313]\n",
            "[Epoch 55: batch 220/265] [d_loss: 0.049352] [g_loss: 0.029116]\n",
            "[Epoch 55: batch 230/265] [d_loss: 0.213349] [g_loss: 0.031070]\n",
            "[Epoch 55: batch 240/265] [d_loss: 0.050439] [g_loss: 0.035965]\n",
            "[Epoch 55: batch 250/265] [d_loss: 0.062709] [g_loss: 0.034394]\n",
            "[Epoch 55: batch 260/265] [d_loss: 0.077649] [g_loss: 0.033478]\n",
            "\n",
            "Saved trained model in /content/drive/MyDrive/SRDRM/checkpoints/USR_4x/srdrm-gan\n",
            "\n",
            "[Epoch 56: batch 6/265] [d_loss: 0.007007] [g_loss: 0.031191]\n",
            "[Epoch 56: batch 16/265] [d_loss: 0.016058] [g_loss: 0.030284]\n",
            "[Epoch 56: batch 26/265] [d_loss: 0.001359] [g_loss: 0.043556]\n",
            "[Epoch 56: batch 36/265] [d_loss: 0.569893] [g_loss: 0.022169]\n",
            "[Epoch 56: batch 46/265] [d_loss: 0.027525] [g_loss: 0.030917]\n",
            "[Epoch 56: batch 56/265] [d_loss: 0.130796] [g_loss: 0.027331]\n",
            "[Epoch 56: batch 66/265] [d_loss: 0.354117] [g_loss: 0.029575]\n",
            "[Epoch 56: batch 76/265] [d_loss: 0.001975] [g_loss: 0.031613]\n",
            "[Epoch 56: batch 86/265] [d_loss: 0.002959] [g_loss: 0.039094]\n",
            "[Epoch 56: batch 96/265] [d_loss: 0.004389] [g_loss: 0.025054]\n",
            "[Epoch 56: batch 106/265] [d_loss: 0.014464] [g_loss: 0.029760]\n",
            "[Epoch 56: batch 116/265] [d_loss: 0.387950] [g_loss: 0.023635]\n",
            "[Epoch 56: batch 126/265] [d_loss: 0.162946] [g_loss: 0.027418]\n",
            "[Epoch 56: batch 136/265] [d_loss: 0.019444] [g_loss: 0.027059]\n",
            "[Epoch 56: batch 146/265] [d_loss: 0.014004] [g_loss: 0.027766]\n",
            "[Epoch 56: batch 156/265] [d_loss: 0.003446] [g_loss: 0.025884]\n",
            "[Epoch 56: batch 166/265] [d_loss: 0.036908] [g_loss: 0.032142]\n",
            "[Epoch 56: batch 176/265] [d_loss: 0.036314] [g_loss: 0.029332]\n",
            "[Epoch 56: batch 186/265] [d_loss: 0.168405] [g_loss: 0.030864]\n",
            "[Epoch 56: batch 196/265] [d_loss: 0.011588] [g_loss: 0.027732]\n",
            "[Epoch 56: batch 206/265] [d_loss: 0.000458] [g_loss: 0.029260]\n",
            "[Epoch 56: batch 216/265] [d_loss: 0.065409] [g_loss: 0.024836]\n",
            "[Epoch 56: batch 226/265] [d_loss: 0.008194] [g_loss: 0.026301]\n",
            "[Epoch 56: batch 236/265] [d_loss: 0.000864] [g_loss: 0.031169]\n",
            "[Epoch 56: batch 246/265] [d_loss: 0.419250] [g_loss: 0.028359]\n",
            "[Epoch 56: batch 256/265] [d_loss: 0.052700] [g_loss: 0.027684]\n",
            "[Epoch 57: batch 2/265] [d_loss: 0.138731] [g_loss: 0.026944]\n",
            "[Epoch 57: batch 12/265] [d_loss: 0.067299] [g_loss: 0.023962]\n",
            "[Epoch 57: batch 22/265] [d_loss: 0.024196] [g_loss: 0.034915]\n",
            "[Epoch 57: batch 32/265] [d_loss: 0.000599] [g_loss: 0.032302]\n",
            "[Epoch 57: batch 42/265] [d_loss: 0.066697] [g_loss: 0.026583]\n",
            "[Epoch 57: batch 52/265] [d_loss: 0.004371] [g_loss: 0.030961]\n",
            "[Epoch 57: batch 62/265] [d_loss: 0.005686] [g_loss: 0.030448]\n",
            "[Epoch 57: batch 72/265] [d_loss: 0.118583] [g_loss: 0.024958]\n",
            "[Epoch 57: batch 82/265] [d_loss: 0.113817] [g_loss: 0.029759]\n",
            "[Epoch 57: batch 92/265] [d_loss: 0.007344] [g_loss: 0.030169]\n",
            "[Epoch 57: batch 102/265] [d_loss: 0.001545] [g_loss: 0.027907]\n",
            "[Epoch 57: batch 112/265] [d_loss: 0.020781] [g_loss: 0.033720]\n",
            "[Epoch 57: batch 122/265] [d_loss: 0.013226] [g_loss: 0.026821]\n",
            "[Epoch 57: batch 132/265] [d_loss: 0.033656] [g_loss: 0.031724]\n",
            "[Epoch 57: batch 142/265] [d_loss: 0.005005] [g_loss: 0.026993]\n",
            "[Epoch 57: batch 152/265] [d_loss: 0.000644] [g_loss: 0.030111]\n",
            "[Epoch 57: batch 162/265] [d_loss: 0.000683] [g_loss: 0.024610]\n",
            "[Epoch 57: batch 172/265] [d_loss: 0.397954] [g_loss: 0.028310]\n",
            "[Epoch 57: batch 182/265] [d_loss: 0.000548] [g_loss: 0.024760]\n",
            "[Epoch 57: batch 192/265] [d_loss: 0.000950] [g_loss: 0.028586]\n",
            "[Epoch 57: batch 202/265] [d_loss: 0.011524] [g_loss: 0.029484]\n",
            "[Epoch 57: batch 212/265] [d_loss: 0.004245] [g_loss: 0.025914]\n",
            "[Epoch 57: batch 222/265] [d_loss: 0.165587] [g_loss: 0.026681]\n",
            "[Epoch 57: batch 232/265] [d_loss: 0.198693] [g_loss: 0.031338]\n",
            "[Epoch 57: batch 242/265] [d_loss: 0.002499] [g_loss: 0.033229]\n",
            "[Epoch 57: batch 252/265] [d_loss: 0.010949] [g_loss: 0.034756]\n",
            "[Epoch 57: batch 262/265] [d_loss: 0.032280] [g_loss: 0.031213]\n",
            "[Epoch 58: batch 8/265] [d_loss: 0.273452] [g_loss: 0.037007]\n",
            "[Epoch 58: batch 18/265] [d_loss: 0.059254] [g_loss: 0.024800]\n",
            "[Epoch 58: batch 28/265] [d_loss: 0.014333] [g_loss: 0.024655]\n",
            "[Epoch 58: batch 38/265] [d_loss: 0.011999] [g_loss: 0.029212]\n",
            "[Epoch 58: batch 48/265] [d_loss: 0.150842] [g_loss: 0.029725]\n",
            "[Epoch 58: batch 58/265] [d_loss: 0.190033] [g_loss: 0.025071]\n",
            "[Epoch 58: batch 68/265] [d_loss: 0.038525] [g_loss: 0.022755]\n",
            "[Epoch 58: batch 78/265] [d_loss: 0.009775] [g_loss: 0.022876]\n",
            "[Epoch 58: batch 88/265] [d_loss: 0.032297] [g_loss: 0.027244]\n",
            "[Epoch 58: batch 98/265] [d_loss: 0.003777] [g_loss: 0.029557]\n",
            "[Epoch 58: batch 108/265] [d_loss: 0.017094] [g_loss: 0.028462]\n",
            "[Epoch 58: batch 118/265] [d_loss: 0.001085] [g_loss: 0.028750]\n",
            "[Epoch 58: batch 128/265] [d_loss: 0.012676] [g_loss: 0.024219]\n",
            "[Epoch 58: batch 138/265] [d_loss: 0.163505] [g_loss: 0.027400]\n",
            "[Epoch 58: batch 148/265] [d_loss: 0.025740] [g_loss: 0.033380]\n",
            "[Epoch 58: batch 158/265] [d_loss: 0.321061] [g_loss: 0.032768]\n",
            "[Epoch 58: batch 168/265] [d_loss: 0.478986] [g_loss: 0.024893]\n",
            "[Epoch 58: batch 178/265] [d_loss: 0.213935] [g_loss: 0.031160]\n",
            "[Epoch 58: batch 188/265] [d_loss: 0.011208] [g_loss: 0.028595]\n",
            "[Epoch 58: batch 198/265] [d_loss: 0.009392] [g_loss: 0.028962]\n",
            "[Epoch 58: batch 208/265] [d_loss: 0.263429] [g_loss: 0.033375]\n",
            "[Epoch 58: batch 218/265] [d_loss: 0.346362] [g_loss: 0.029962]\n",
            "[Epoch 58: batch 228/265] [d_loss: 0.011464] [g_loss: 0.029876]\n",
            "[Epoch 58: batch 238/265] [d_loss: 0.016148] [g_loss: 0.023956]\n",
            "[Epoch 58: batch 248/265] [d_loss: 0.607106] [g_loss: 0.021236]\n",
            "[Epoch 58: batch 258/265] [d_loss: 0.000900] [g_loss: 0.028264]\n",
            "[Epoch 59: batch 4/265] [d_loss: 0.004553] [g_loss: 0.034090]\n",
            "[Epoch 59: batch 14/265] [d_loss: 0.470892] [g_loss: 0.026921]\n",
            "[Epoch 59: batch 24/265] [d_loss: 0.270412] [g_loss: 0.028699]\n",
            "[Epoch 59: batch 34/265] [d_loss: 0.403840] [g_loss: 0.022346]\n",
            "[Epoch 59: batch 44/265] [d_loss: 0.010918] [g_loss: 0.033975]\n",
            "[Epoch 59: batch 54/265] [d_loss: 0.013897] [g_loss: 0.025497]\n",
            "[Epoch 59: batch 64/265] [d_loss: 0.041289] [g_loss: 0.033227]\n",
            "[Epoch 59: batch 74/265] [d_loss: 0.006121] [g_loss: 0.036080]\n",
            "[Epoch 59: batch 84/265] [d_loss: 0.002708] [g_loss: 0.029497]\n",
            "[Epoch 59: batch 94/265] [d_loss: 0.007436] [g_loss: 0.027984]\n",
            "[Epoch 59: batch 104/265] [d_loss: 0.308133] [g_loss: 0.029128]\n",
            "[Epoch 59: batch 114/265] [d_loss: 0.356155] [g_loss: 0.024501]\n",
            "[Epoch 59: batch 124/265] [d_loss: 0.187318] [g_loss: 0.029973]\n",
            "[Epoch 59: batch 134/265] [d_loss: 0.006333] [g_loss: 0.029911]\n",
            "[Epoch 59: batch 144/265] [d_loss: 0.003612] [g_loss: 0.024970]\n",
            "[Epoch 59: batch 154/265] [d_loss: 0.007599] [g_loss: 0.031754]\n",
            "[Epoch 59: batch 164/265] [d_loss: 0.010807] [g_loss: 0.027444]\n",
            "[Epoch 59: batch 174/265] [d_loss: 0.248279] [g_loss: 0.030131]\n",
            "[Epoch 59: batch 184/265] [d_loss: 0.002663] [g_loss: 0.036918]\n",
            "[Epoch 59: batch 194/265] [d_loss: 0.057803] [g_loss: 0.030486]\n",
            "[Epoch 59: batch 204/265] [d_loss: 0.002329] [g_loss: 0.041022]\n",
            "[Epoch 59: batch 214/265] [d_loss: 0.044099] [g_loss: 0.026375]\n",
            "[Epoch 59: batch 224/265] [d_loss: 0.007254] [g_loss: 0.029273]\n",
            "[Epoch 59: batch 234/265] [d_loss: 0.206918] [g_loss: 0.028352]\n",
            "[Epoch 59: batch 244/265] [d_loss: 0.045789] [g_loss: 0.029871]\n",
            "[Epoch 59: batch 254/265] [d_loss: 0.261530] [g_loss: 0.030548]\n",
            "[Epoch 59: batch 264/265] [d_loss: 0.073609] [g_loss: 0.039573]\n",
            "\n",
            "Saved trained model in /content/drive/MyDrive/SRDRM/checkpoints/USR_4x/srdrm-gan\n",
            "\n",
            "[Epoch 60: batch 10/265] [d_loss: 0.003234] [g_loss: 0.035540]\n",
            "[Epoch 60: batch 20/265] [d_loss: 0.450452] [g_loss: 0.026237]\n",
            "[Epoch 60: batch 30/265] [d_loss: 0.025811] [g_loss: 0.028127]\n",
            "[Epoch 60: batch 40/265] [d_loss: 0.059890] [g_loss: 0.026420]\n",
            "[Epoch 60: batch 50/265] [d_loss: 0.015943] [g_loss: 0.035307]\n",
            "[Epoch 60: batch 60/265] [d_loss: 0.002543] [g_loss: 0.028295]\n",
            "[Epoch 60: batch 70/265] [d_loss: 0.005757] [g_loss: 0.036321]\n",
            "[Epoch 60: batch 80/265] [d_loss: 0.001505] [g_loss: 0.031439]\n",
            "[Epoch 60: batch 90/265] [d_loss: 0.047585] [g_loss: 0.026926]\n",
            "[Epoch 60: batch 100/265] [d_loss: 0.033302] [g_loss: 0.029044]\n",
            "[Epoch 60: batch 110/265] [d_loss: 0.020339] [g_loss: 0.028044]\n",
            "[Epoch 60: batch 120/265] [d_loss: 0.357451] [g_loss: 0.029531]\n",
            "[Epoch 60: batch 130/265] [d_loss: 0.029962] [g_loss: 0.032900]\n",
            "[Epoch 60: batch 140/265] [d_loss: 0.441832] [g_loss: 0.032272]\n",
            "[Epoch 60: batch 150/265] [d_loss: 0.073044] [g_loss: 0.034099]\n",
            "[Epoch 60: batch 160/265] [d_loss: 0.473821] [g_loss: 0.025611]\n",
            "[Epoch 60: batch 170/265] [d_loss: 0.003587] [g_loss: 0.028953]\n",
            "[Epoch 60: batch 180/265] [d_loss: 0.008666] [g_loss: 0.029365]\n",
            "[Epoch 60: batch 190/265] [d_loss: 0.020780] [g_loss: 0.028497]\n",
            "[Epoch 60: batch 200/265] [d_loss: 0.010065] [g_loss: 0.032500]\n",
            "[Epoch 60: batch 210/265] [d_loss: 0.001425] [g_loss: 0.033133]\n",
            "[Epoch 60: batch 220/265] [d_loss: 0.002770] [g_loss: 0.030746]\n",
            "[Epoch 60: batch 230/265] [d_loss: 0.018788] [g_loss: 0.027844]\n",
            "[Epoch 60: batch 240/265] [d_loss: 0.024137] [g_loss: 0.030256]\n",
            "[Epoch 60: batch 250/265] [d_loss: 0.020056] [g_loss: 0.033529]\n",
            "[Epoch 60: batch 260/265] [d_loss: 0.003186] [g_loss: 0.034856]\n",
            "[Epoch 61: batch 6/265] [d_loss: 0.218785] [g_loss: 0.030539]\n",
            "[Epoch 61: batch 16/265] [d_loss: 0.003521] [g_loss: 0.030661]\n",
            "[Epoch 61: batch 26/265] [d_loss: 0.012939] [g_loss: 0.038897]\n",
            "[Epoch 61: batch 36/265] [d_loss: 0.362062] [g_loss: 0.026673]\n",
            "[Epoch 61: batch 46/265] [d_loss: 0.011400] [g_loss: 0.033482]\n",
            "[Epoch 61: batch 56/265] [d_loss: 0.018425] [g_loss: 0.025788]\n",
            "[Epoch 61: batch 66/265] [d_loss: 0.090651] [g_loss: 0.029685]\n",
            "[Epoch 61: batch 76/265] [d_loss: 0.007521] [g_loss: 0.033273]\n",
            "[Epoch 61: batch 86/265] [d_loss: 0.000576] [g_loss: 0.037229]\n",
            "[Epoch 61: batch 96/265] [d_loss: 0.010524] [g_loss: 0.024086]\n",
            "[Epoch 61: batch 106/265] [d_loss: 0.000942] [g_loss: 0.032948]\n",
            "[Epoch 61: batch 116/265] [d_loss: 0.037321] [g_loss: 0.023487]\n",
            "[Epoch 61: batch 126/265] [d_loss: 0.291618] [g_loss: 0.027689]\n",
            "[Epoch 61: batch 136/265] [d_loss: 0.070263] [g_loss: 0.026742]\n",
            "[Epoch 61: batch 146/265] [d_loss: 0.005766] [g_loss: 0.027670]\n",
            "[Epoch 61: batch 156/265] [d_loss: 0.037558] [g_loss: 0.025074]\n",
            "[Epoch 61: batch 166/265] [d_loss: 0.045549] [g_loss: 0.034439]\n",
            "[Epoch 61: batch 176/265] [d_loss: 0.011938] [g_loss: 0.028188]\n",
            "[Epoch 61: batch 186/265] [d_loss: 0.079002] [g_loss: 0.027082]\n",
            "[Epoch 61: batch 196/265] [d_loss: 0.065798] [g_loss: 0.030040]\n",
            "[Epoch 61: batch 206/265] [d_loss: 0.002271] [g_loss: 0.026414]\n",
            "[Epoch 61: batch 216/265] [d_loss: 0.014840] [g_loss: 0.022896]\n",
            "[Epoch 61: batch 226/265] [d_loss: 0.001662] [g_loss: 0.024640]\n",
            "[Epoch 61: batch 236/265] [d_loss: 0.241907] [g_loss: 0.034629]\n",
            "[Epoch 61: batch 246/265] [d_loss: 0.000747] [g_loss: 0.027305]\n",
            "[Epoch 61: batch 256/265] [d_loss: 0.000567] [g_loss: 0.033050]\n",
            "[Epoch 62: batch 2/265] [d_loss: 0.096029] [g_loss: 0.028334]\n",
            "[Epoch 62: batch 12/265] [d_loss: 0.535944] [g_loss: 0.026696]\n",
            "[Epoch 62: batch 22/265] [d_loss: 0.018920] [g_loss: 0.034142]\n",
            "[Epoch 62: batch 32/265] [d_loss: 0.011635] [g_loss: 0.034871]\n",
            "[Epoch 62: batch 42/265] [d_loss: 0.004789] [g_loss: 0.027592]\n",
            "[Epoch 62: batch 52/265] [d_loss: 0.014195] [g_loss: 0.029250]\n",
            "[Epoch 62: batch 62/265] [d_loss: 0.007625] [g_loss: 0.032693]\n",
            "[Epoch 62: batch 72/265] [d_loss: 0.475840] [g_loss: 0.029852]\n",
            "[Epoch 62: batch 82/265] [d_loss: 0.059850] [g_loss: 0.029770]\n",
            "[Epoch 62: batch 92/265] [d_loss: 0.022849] [g_loss: 0.028928]\n",
            "[Epoch 62: batch 102/265] [d_loss: 0.012727] [g_loss: 0.032159]\n",
            "[Epoch 62: batch 112/265] [d_loss: 0.002786] [g_loss: 0.031915]\n",
            "[Epoch 62: batch 122/265] [d_loss: 0.002098] [g_loss: 0.023982]\n",
            "[Epoch 62: batch 132/265] [d_loss: 0.003903] [g_loss: 0.032556]\n",
            "[Epoch 62: batch 142/265] [d_loss: 0.004063] [g_loss: 0.023676]\n",
            "[Epoch 62: batch 152/265] [d_loss: 0.045152] [g_loss: 0.029378]\n",
            "[Epoch 62: batch 162/265] [d_loss: 0.062099] [g_loss: 0.023946]\n",
            "[Epoch 62: batch 172/265] [d_loss: 0.004735] [g_loss: 0.033400]\n",
            "[Epoch 62: batch 182/265] [d_loss: 0.000561] [g_loss: 0.024930]\n",
            "[Epoch 62: batch 192/265] [d_loss: 0.000966] [g_loss: 0.027489]\n",
            "[Epoch 62: batch 202/265] [d_loss: 0.006034] [g_loss: 0.028421]\n",
            "[Epoch 62: batch 212/265] [d_loss: 0.000725] [g_loss: 0.024900]\n",
            "[Epoch 62: batch 222/265] [d_loss: 0.008689] [g_loss: 0.023085]\n",
            "[Epoch 62: batch 232/265] [d_loss: 0.185524] [g_loss: 0.031822]\n",
            "[Epoch 62: batch 242/265] [d_loss: 0.000112] [g_loss: 0.056895]\n",
            "[Epoch 62: batch 252/265] [d_loss: 0.000055] [g_loss: 0.046476]\n",
            "[Epoch 62: batch 262/265] [d_loss: 0.000119] [g_loss: 0.039359]\n",
            "[Epoch 63: batch 8/265] [d_loss: 0.000642] [g_loss: 0.041766]\n",
            "[Epoch 63: batch 18/265] [d_loss: 0.001742] [g_loss: 0.028085]\n",
            "[Epoch 63: batch 28/265] [d_loss: 0.000201] [g_loss: 0.029400]\n",
            "[Epoch 63: batch 38/265] [d_loss: 0.000672] [g_loss: 0.035777]\n",
            "[Epoch 63: batch 48/265] [d_loss: 0.000273] [g_loss: 0.034589]\n",
            "[Epoch 63: batch 58/265] [d_loss: 0.006065] [g_loss: 0.025798]\n",
            "[Epoch 63: batch 68/265] [d_loss: 0.000407] [g_loss: 0.030408]\n",
            "[Epoch 63: batch 78/265] [d_loss: 0.038183] [g_loss: 0.028029]\n",
            "[Epoch 63: batch 88/265] [d_loss: 0.000357] [g_loss: 0.030692]\n",
            "[Epoch 63: batch 98/265] [d_loss: 0.018152] [g_loss: 0.030572]\n",
            "[Epoch 63: batch 108/265] [d_loss: 0.015507] [g_loss: 0.028793]\n",
            "[Epoch 63: batch 118/265] [d_loss: 0.011718] [g_loss: 0.029273]\n",
            "[Epoch 63: batch 128/265] [d_loss: 0.008647] [g_loss: 0.028836]\n",
            "[Epoch 63: batch 138/265] [d_loss: 0.001516] [g_loss: 0.027075]\n",
            "[Epoch 63: batch 148/265] [d_loss: 0.425768] [g_loss: 0.035568]\n",
            "[Epoch 63: batch 158/265] [d_loss: 0.010046] [g_loss: 0.032124]\n",
            "[Epoch 63: batch 168/265] [d_loss: 0.442547] [g_loss: 0.028877]\n",
            "[Epoch 63: batch 178/265] [d_loss: 0.005125] [g_loss: 0.031712]\n",
            "[Epoch 63: batch 188/265] [d_loss: 0.072583] [g_loss: 0.030421]\n",
            "[Epoch 63: batch 198/265] [d_loss: 0.002097] [g_loss: 0.030995]\n",
            "[Epoch 63: batch 208/265] [d_loss: 0.031765] [g_loss: 0.035727]\n",
            "[Epoch 63: batch 218/265] [d_loss: 0.170484] [g_loss: 0.036145]\n",
            "[Epoch 63: batch 228/265] [d_loss: 0.000498] [g_loss: 0.030360]\n",
            "[Epoch 63: batch 238/265] [d_loss: 0.147718] [g_loss: 0.024250]\n",
            "[Epoch 63: batch 248/265] [d_loss: 0.481180] [g_loss: 0.026815]\n",
            "[Epoch 63: batch 258/265] [d_loss: 0.064336] [g_loss: 0.032861]\n",
            "\n",
            "Saved trained model in /content/drive/MyDrive/SRDRM/checkpoints/USR_4x/srdrm-gan\n",
            "\n",
            "[Epoch 64: batch 4/265] [d_loss: 0.001113] [g_loss: 0.034441]\n",
            "[Epoch 64: batch 14/265] [d_loss: 0.405653] [g_loss: 0.026691]\n",
            "[Epoch 64: batch 24/265] [d_loss: 0.002404] [g_loss: 0.029591]\n",
            "[Epoch 64: batch 34/265] [d_loss: 0.475010] [g_loss: 0.028311]\n",
            "[Epoch 64: batch 44/265] [d_loss: 0.041970] [g_loss: 0.033600]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}